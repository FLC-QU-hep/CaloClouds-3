[![pipeline status](https://gitlab.desy.de/ftx-sft/generative/point-cloud-diffusion/badges/main/pipeline.svg?ignore_skipped=true)](https://gitlab.desy.de/ftx-sft/generative/point-cloud-diffusion/-/commits/main) 
[![coverage](https://gitlab.desy.de/ftx-sft/generative/point-cloud-diffusion/badges/main/coverage.svg)](https://gitlab.desy.de/ftx-sft/generative/point-cloud-diffusion/-/commits/main) 
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)


# Rolling implementation of CaloClouds and friends

Spawned the CaloClouds model published in ([arXiv:2305.04847](https://arxiv.org/abs/2305.04847)).
and also *CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation* ([arXiv:2309.05704](https://arxiv.org/abs/2309.05704)).


Not sure how to do something with git or gitlab? Please start by checking the docs here; [`gitlab.desy.de/ftx-sft/Documentation/-/tree/master/git`](https://gitlab.desy.de/ftx-sft/Documentation/-/tree/master/git)

---

## Repository structure

The repository is structured around a module called `pointcloud`, this is the bulk of the code.
The entry points, however, are all in the `scripts` folder.

- `pointcloud`: The parent module for code to be imported.
    - `configs.py`: A symlink to the default configs file. Changes are ignored by git by default.
    - `config_varients`: Library of configs files that `configs.py` could point to. You can also import them directly.
    - `data`: Data reading and writing, classes to hold data structures.
    - `evaluation`: Module containing functions for preparing data and calculating statistics for model evaluation.
    - `metadata`: Contains a set of subfolders (and symlinks to subfolders) specifying the metadata for each dataset. For example, detector geometry. See [`metadata.md`](./metadata.md) for more details.
    - `models`: Model classes themselves, and some helper code specific to models.
    - `anomalies`: Code relating to anomaly detection in the data.
    - `utils`: Misc code that is useful at multiple points.
- `scripts`: Code that doesn't get imported. Every file below this directory is an entry point.
    - `pointcloud`: Symlink back to the parent module, so that scripts can import code from it without messing with the path.
    - `evaluation`: Scripts for plotting and comparing the data generated by the models to the ground truth.
    - `jobsubmission`: Scripts for submitting jobs on cluster computers.
- `programming_utils`: Meta level utilities for maintaining the repository.
- `test`: The unit tests.
    - `pointcloud`: Symlink back to the parent module, so that tests can import code from it without messing with the path.
    - `scripts`: Symlink to scripts module; some scripts there get regression tests and thus need to be imported.
    - `programming_utils`: Symlink to programming utilities module, to allow for CI to check the repository is well maintained.
- `externals`: other repos, containing other packages that are treated as submodules for convenience.


---

## Getting started

Configs are stored in [`pointcloud/config_varients/`](./pointcloud/config_varients/), and the default choice is determined by what the soft link `pointcloud/configs.py` is pointing to.
To make a new config;
- Make the base file with `cd pointcloud && cp config_varients/default.py config_varients/my_funky_new_config_name.py`.
- Edit the funky new config to your taste.
    It's also possible to `from .config_varients import default` to inherit from that `Configs` class, and change only what you need.
    See [`config_varients/wish.py`](./pointcloud/config_varients/wish.py) for an example of that.
- Remove the existing symbolic link with `rm configs.py`
- Link your config `ln -s config_varients/my_funky_new_config_name.py configs.py`

The teacher model is trained using [`scripts/main.py`](./scripts/main.py), and the student model is trained with [`scripts/cd.py`](./scripts/cd.py).
It should be called as a script, like; `python3 script/main.py`.

The Shower Flow (to predict energy and hist per layer) is trained via the notebook [`scripts/ShowerFlow.ipynb`](./scripts/ShowerFlow.ipynb).
It will work in a jupyter notebook.

The polynomial fits for the occupancy calculations are performed in [`scripts/occupancy_scale.ipynb`](./scripts/occupancy_scale.ipynb).

An outline of the sampling process for both CaloClouds II and CaloClouds II (CM) can be found in [`pointcloud/evaluation/generate.py`](./pointcloud/evaluation/generate.py).

The timing of the models is benchmarked with [`scripts/timing.py`](./scripts/timing.py), also called as a script.

---

The training dataset is available under the link: https://syncandshare.desy.de/index.php/s/XfDwx33ryERwPdi

But if you are connected to `beegfs` you can also find the relevant goodies in Anatolii's directories.
For example;
`/beegfs/desy/user/akorol/data/calo-clouds/hdf5/high_granular_grid/train/10-90GeV_x36_grid_regular_524k_float32.hdf5`

Also, if you would be interested in generating some data, useful code can be found at [`gitlab.desy.de/ftx-sft/generative/data_production`](https://gitlab.desy.de/ftx-sft/generative/data_production).

---

### Information on Wish

One model in this dataset is a classical statistics model; the "wish" model.
It lives in [`pointcloud/models/wish.py`](./pointcloud/models/wish.py).
While it is possible to "train" this model (the parameters all have gradient), it's also possible to just set it using
statistics accumulated from the whole dataset.

The simplest possible way to do this is;
```python
# import an apropreate config
from pointcloud.config_varients.wish import Configs
configs = Configs()
configs.dataset_path  # check that the configs points to your dataset, etc.

from pointcloud.models.wish import accumulate_and_load_wish, load_wish_from_accumulator
wish_model, accumulated_data = accumulate_and_load_wish(configs)

wish_model.save("path/to/save/model.pt")
# we don't have to save the accumulated_data, but it can be used
# to make another wish model without reaccumulating if needed
accumulated_data.save("path/to/save/accumulated.h5")

# change some model hyperparameters
configs.poly_degree = 5
configs.fit_attempts = 50
new_model = load_wish_from_accumulator("path/to/save/accumulated.h5", configs)
```
---

## Code references


- The code for training the score-based model is based on: https://github.com/crowsonkb/k-diffusion
- The consistency distillation is based on: https://github.com/openai/consistency_models/
- The PointWise Net is adapted from: https://github.com/luost26/diffusion-point-cloud
- Code base for our CaloClouds (1) model: https://github.com/FLC-QU-hep/CaloClouds
- Code used for lazy operations on h5py arrays is from: https://github.com/catalystneuro/lazy_ops It is not installed from a repository because a bug fix was needed, and the maintainer cannot be reached. For simplicities sake, the relevant patched file is in `pointcloud/externals/lazy_ops` along with the licence file for the code.
