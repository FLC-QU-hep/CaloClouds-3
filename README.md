[![arXiv](https://img.shields.io/badge/arXiv-2511.01460-<COLOR>.svg)](https://arxiv.org/abs/2511.01460)
[![Run python tests](https://github.com/FLC-QU-hep/CaloClouds-3/actions/workflows/ci.yml/badge.svg)](https://github.com/FLC-QU-hep/CaloClouds-3/actions/workflows/ci.yml)
[![cov](https://FLC-QU-hep.github.io/CaloClouds-3/badges/coverage.svg)](https://github.com/FLC-QU-hep/CaloClouds-3/actions)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)


# CaloClouds 3

### Thorsten Buss<sup>1,2</sup>, Henry~Day-Hall<sup>2</sup>, Frank Gaede<sup>2</sup>, Gregor Kasieczka<sup>1</sup>, Katja Kr√ºger<sup>2</sup>, Anatolii Korol<sup>2</sup>, Thomas Madlener<sup>2</sup>, Peter McKeown<sup>3</sup>, Martina Mozzanica<sup>1</sup>, Lorenzo Valente<sup>1</sup>
<sup>1</sup>DESY, <sup>2</sup>University of Hamburg, <sup>3</sup>CERN

Code used for training and intermediate assessments of the CaloClouds3 model,
as published in ([arXiv:2511.01460](https://arxiv.org/pdf/2511.01460)).
This model is a successor to the original CaloClouds model published in ([arXiv:2305.04847](https://arxiv.org/abs/2305.04847)).
and also *CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular Calorimeter Simulation* ([arXiv:2309.05704](https://arxiv.org/abs/2309.05704)).

Its purpose is to provide a fast simulation of a photon shower in a calorimeter, as depicted in the image below

![Photon shower at 70 GeV](./assets/cc_e70_s0_idx2241_wbg.png)

For more detailed information, please see the paper.
If you just want to produce photon showers with this model, an inference only setup is available in the [Quantum Universe Model Zoo](https://hmz-hub.desy.de).


The repository was forked from the CaloClouds 2 repository, ([github.com/FLC-QU-hep/CaloClouds-2](https://github.com/FLC-QU-hep/CaloClouds-2)).
It has been developed iteratively between 2 primary developers, Anatolii Korol and Henry Day-Hall,
 with contributions from Thorsten Buss and Lorenzo Valente. 


---

## Repository structure

The repository is structured around a module called `pointcloud`, this is the bulk of the code.
The entry points, however, are all in the `scripts` folder.

- `pointcloud`: The parent module for code to be imported.
    - `configs.py`: A symlink to the default configs file. Changes are ignored by git by default.
    - `config_varients`: Library of configs files that `configs.py` could point to. You can also import them directly.
    - `data`: Data reading and writing, classes to hold data structures.
    - `evaluation`: Module containing functions for preparing data and calculating statistics for model evaluation.
    - `metadata`: Contains a set of subfolders (and symlinks to subfolders) specifying the metadata for each dataset. For example, detector geometry. See [`metadata.md`](./metadata.md) for more details.
    - `models`: Model classes themselves, and some helper code specific to models.
    - `utils`: Misc code that is useful at multiple points.
- `scripts`: Code that doesn't get imported. Every file below this directory is an entry point.
    - `pointcloud`: Symlink back to the parent module, so that scripts can import code from it without messing with the path.
    - `evaluation`: Scripts for plotting and comparing the data generated by the models to the ground truth.
    - `jobsubmission`: Scripts for submitting jobs on cluster computers.
- `programming_utils`: Meta level utilities for maintaining the repository.
- `test`: The unit tests.
    - `pointcloud`: Symlink back to the parent module, so that tests can import code from it without messing with the path.
    - `scripts`: Symlink to scripts module; some scripts there get regression tests and thus need to be imported.
    - `programming_utils`: Symlink to programming utilities module, to allow for CI to check the repository is well maintained.


---

## Getting started

Clone this repository with

```bash
git clone https://github.com/FLC-QU-hep/CaloClouds-3.git
```

Optionally, setup an environment to contain the requirements, this can be done with conda or virtualenv.

```bash
conda env create -n "caloclouds3" python=3.12.7
conda activate caloclouds3
conda install pip
```
or 

```bash
python3 -m venv caloclouds3
source caloclouds3/bin/activate
```

Some requirements don't install as well as others, so it's best to install them "manually"

```bash
pip install setuptools==69.0.3
pip install -r requirements.txt
```

Now in principle, the code is ready to go.
If you have issues with the torch installation it can be worth doing that one manually also,
it must match your CUDA version to work on GPU.


### Local setup

Commonly changed settings are stored in a `Configs` object.
Most notably, file paths to data and models are in the `Configs` object.
So a good strategy would be to start out using the default configs, then when you get file-path related errors,
make your own configs with the right file paths for your machine.
These will also include the paths that determine the write locations for models that are created.
Configs are stored in [`pointcloud/config_varients/`](./pointcloud/config_varients/), and the default choice is determined by what the soft link `pointcloud/configs.py` is pointing to.

To make your own config;
- Make the base file with `cd pointcloud && cp config_varients/example.py config_varients/my_funky_new_config_name.py`.
- Edit the funky new config to your taste.
    The elements of the config that you are most likely to need to change are present in [`config_varients/example.py`](./pointcloud/config_varients/example.py).
    To see all possible entries in a config, have a look at [`pointcloud/config_varients/default.py`](./pointcloud/config_varients/default.py) and
    [`pointcloud/config_varients/caloclouds_3.py`](./pointcloud/config_varients/caloclouds_3.py).
- Remove the existing symbolic link with `rm configs.py`
- Link your config `ln -s config_varients/my_funky_new_config_name.py configs.py`

Then, if you are training the CaloClouds model family, you need to train two (or three) models; the teacher diffusion model (and optionally the distilled student model)
and also the Shower Flow model.
The teacher model is trained using [`scripts/training/diffusion.py`](./scripts/training/diffusion.py), and the student model is trained with [`scripts/training/cd.py`](./scripts/training/cd.py).
It should be called as a script, like; `python3 script/training/diffusion.py`.
First train the teacher model with `python3 scripts/training/diffusion.py`,
then add the location of the created teacher model to your configs file,
then train the student model with `python3 scripts/training/cd.py`.
Both of them will use the configs that are softlinked at `pointcloud/configs.py`, so be sure they have the right file paths.


The Shower Flow (to predict energy and hist per layer) is trained via the script [`scripts/training/ShowerFlow.py`](./scripts/training/ShowerFlow.py).

The polynomial fits for the occupancy calculations are performed in [`scripts/training/calcluate_coef.ipynb`](./scripts/training/calcluate_coef.ipynb); or you can do calculations manually on the models produced.

An outline of the sampling process for CaloClouds 3 can be found in [`scripts/generate.py`](./scripts/generate.py).

---

The training dataset is available under the link: https://syncandshare.desy.de/index.php/s/XfDwx33ryERwPdi

But if you are connected to `/data/dust` at DESY you can also find the relevant goodies in the `ilc`'s group directories;
For example;
`/data/dust/group/ilc/sft-ml/datasets/sim-E1261AT600AP180-180/`

Also, if you would be interested in generating some data, useful code can be found at [`gitlab.desy.de/ftx-sft/generative/data_production`](https://gitlab.desy.de/ftx-sft/generative/data_production).

---

### Evaluating models

There are tools for quickly generating some standard metrics for comparing model performance.
The functions and classes in [`pointcloud/evaluation/bin_standard_metrics.py`](./pointcloud/evaluation/bin_standard_metrics.py) work with the script [`scripts/evaluation/create_standard_metrics.py`](./scripts/evaluation/create_standard_metrics.py) to generate and save some kinematic bins for standard variables,
which are saved as `.npz` in the log directory (as specified by the configs).
So to make pretty plots, first check your model is in (see below) [`scripts/evaluation/create_standard_metrics.py`](./scripts/evaluation/create_standard_metrics.py) and run it.
Then use [`scripts/plotting/standard_metrics.ipynb`](scripts/plotting/standard_metrics.ipynb) to visualise the performance of the model.
[`scripts/plotting/standard_metrics.ipynb`](./scripts/plotting/standard_metrics.ipynb) also has some reference values for comparison.

If you have a new model, and it has been trained for the ILD detector, it would be good to add it to this comparison. To add a new model;

1. Ensure that the `gen_condE_showers_batch` function in [`pointcloud/utils/gen_utils.py`](./pointcloud/utils/gen_utils.py) can correctly generate data for your model. This is what will sample your model. Potentially adding a new `inner_batch_func`.
2. Add a function to [`pointcloud/evaluation/bin_standard_metrics.py`](./pointcloud/evaluation/bin_standard_metrics.py) in the same format as the `get_caloclouds_models` or `get_wish_models` functions. This is where your model is loaded.
3. In [`scripts/evaluation/create_standard_metrics.py`](./scripts/evaluation/create_standard_metrics.py) import and call your model creation function and add the new entries to the `models` dict.
4. In [`scripts/plotting/standard_metrics.ipynb`](./scripts/plotting/standard_metrics.ipynb) add your model name to the `model_names` list, so that it is picked up in the plots.
5. Rerun [`scripts/evaluation/create_standard_metrics.py`](./scripts/evaluation/create_standard_metrics.py) and [`scripts/plotting/standard_metrics.ipynb`](./scripts/plotting/standard_metrics.ipynb).

Done.

The timing of the models is benchmarked with [`scripts/evaluation/timing.py`](./scripts/evaluation/timing.py), also called as a script.


## Code references


- The code for training the score-based model is based on: https://github.com/crowsonkb/k-diffusion
- The consistency distillation is based on: https://github.com/openai/consistency_models/
- The PointWise Net is adapted from: https://github.com/luost26/diffusion-point-cloud
- Code base for our CaloClouds (1) model: https://github.com/FLC-QU-hep/CaloClouds
