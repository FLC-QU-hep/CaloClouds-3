{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46a902d-15bf-4f26-8769-6d2e6dbd78fc",
   "metadata": {},
   "source": [
    "# Visulising the backbone\n",
    "\n",
    "Backbone == showerflow\n",
    "\n",
    "Going to make a sequence of plots to see the backbone performance compare to the data.\n",
    "\n",
    "## version 4.6\n",
    "\n",
    "Currently; fits used in backbone are Weibull (experimental) for n-hits, 1D log normal for mean point energy. The fits used in the layers are a 2D gauss for the lateral shape, and a LogNormal for the distribution of energies within an event.\n",
    "Fits are now polynomial, 3rd order, with respect to the incident energy.\n",
    "\n",
    "## Content of notebook\n",
    "\n",
    "Histograms;\n",
    "- Total predicted energy\n",
    "   * Summed over all layers\n",
    "   * In each layer\n",
    "- Total predicted hits\n",
    "   * Summed over all layers\n",
    "   * In each layer\n",
    "\n",
    "Scatter plots;\n",
    "\n",
    "- Incident energy v.s. predicted visible energy\n",
    "   * Summed over all layers\n",
    "   * In each layer\n",
    "- Incident energy v.s. predicted hits\n",
    "   * Summed over all layers\n",
    "   * In each layer\n",
    "- For fixed incident energy\n",
    "   * Distribution of visible energy\n",
    "       * Summed over all layers\n",
    "       * In each layer\n",
    "   * Distribution of hits\n",
    "       * Summed over all layers\n",
    "       * In each layer\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279dbda7-a322-4ff0-8bc4-382587e15095",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "\n",
    "Start by getting g4 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa94dd22-44f1-484b-8794-0e692d8dbbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Reading section 0 of 10\n",
      "Will read 299893 events of 2998932 in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2999/2999 [1:00:14<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /data/dust/user/dayhallh/point-cloud-diffusion-logs/dataset_accumulators/sim-E1261AT600AP180-180_file_Allslcio/10_sections/section_0.h5\n"
     ]
    }
   ],
   "source": [
    "from pointcloud.utils.stats_accumulator import StatsAccumulator, HighLevelStats, read_section, save_location\n",
    "from pointcloud.models.wish import load_wish_from_accumulator\n",
    "import numpy as np\n",
    "from pointcloud.config_varients.caloclouds_3_simple_shower import Configs\n",
    "from pointcloud.utils.metadata import Metadata\n",
    "from pointcloud.data.dataset import PointCloudDataset\n",
    "import os\n",
    "\n",
    "config = Configs()\n",
    "config.poly_degree = 3\n",
    "meta = Metadata(config)\n",
    "\n",
    "remake_wish = False\n",
    "redo_wish_data = False\n",
    "redo_g4_data = False\n",
    "\n",
    "energy_lower = 0.0 # 0.25\n",
    "energy_upper = np.inf\n",
    "varient = \"\"  # could also use \"_0p25\", or \"_0\"\n",
    "#file_path = f\"../../../point-cloud-diffusion-logs/wish/dataset_accumulators/10-90GeV_x36_grid_regular_524k_float32_filtered/from_10{varient}.h5\"\n",
    "#file_path = f\"../../../point-cloud-diffusion-logs/wish/dataset_accumulators/initial_accumulation.h5\"\n",
    "#file_path = \"/data/dust/user/dayhallh/point-cloud-diffusion-logs/wish/dataset_accumulators/p22_th90_ph90_en10-1/p22_th90_ph90_en10-100_seedAll_all_steps.h5\"\n",
    "file_path = save_location(config, 10, 0)\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Creating\")\n",
    "    read_section(10, 0, config)\n",
    "\n",
    "accumulated_stats = StatsAccumulator.load(file_path)\n",
    "# get bins\n",
    "incident_bins = accumulated_stats.incident_bin_boundaries\n",
    "\n",
    "\n",
    "# Get g4 data\n",
    "points = incident_bins[:-1] + 0.5*(incident_bins[1:] - incident_bins[:-1])\n",
    "non_zero_total_events = np.clip(accumulated_stats.total_events, 1, None)\n",
    "g4_observed_energies = np.sum(accumulated_stats.energy_hist, axis=(1, 2, 3))/non_zero_total_events\n",
    "g4_observed_energies = g4_observed_energies[1:-1]\n",
    "g4_observed_hits = np.sum(accumulated_stats.counts_hist, axis=(1, 2, 3))/non_zero_total_events\n",
    "g4_observed_hits = g4_observed_hits[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80807dde-8101-4378-9de7-be1681a9a41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb06c7c7-84fc-4611-8581-ae3e416cbdd9",
   "metadata": {},
   "source": [
    "Get the model under comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ff7b4-7b5a-4b50-aa48-9f7faa61455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pointcloud.models.wish import Wish\n",
    "\n",
    "#wish_path = \"../../../point-cloud-diffusion-logs/wish/dataset_accumulators/10-90GeV_x36_grid_regular_524k_float32/wish_from_10.pt\"\n",
    "wish_path = \"../../../point-cloud-diffusion-logs/wish/dataset_accumulators/p22_th90_ph90_en10-1/wish_poly3.pt\"\n",
    "\n",
    "if remake_wish:\n",
    "    #wish_model = Wish(config)\n",
    "    wish_model = load_wish_from_accumulator(file_path, config=config)\n",
    "    wish_model.save(wish_path)\n",
    "\n",
    "wish_model = Wish.load(wish_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe418c89-3335-439b-9a5a-368497348c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = wish_model.layers[0]\n",
    "l0.energy_fit.get_standarddev(20)\n",
    "\n",
    "n_points = len(points)\n",
    "n_layers = len(wish_model.layers)\n",
    "from matplotlib import pyplot as plt\n",
    "from pointcloud.utils.plotting import plot_line_with_devation\n",
    "import numpy as np\n",
    "n_intrest = 5\n",
    "layers_of_intrest = np.linspace(0, n_layers-1, n_intrest).astype(int)\n",
    "\n",
    "nice_hex = [[\"#00E5E3\", \"#8DD7BF\", \"#FF96C5\", \"#FF5768\", \"#FFBF65\"],\n",
    "            [\"#FC6238\", \"#FFD872\", \"#F2D4CC\", \"#E77577\", \"#6C88C4\"],\n",
    "            [\"#C05780\", \"#FF828B\", \"#E7C582\", \"#00B0BA\", \"#0065A2\"],\n",
    "            [\"#00CDAC\", \"#FF6F68\", \"#FFDACC\", \"#FF60A8\", \"#CFF800\"],\n",
    "            [\"#FF5C77\", \"#4DD091\", \"#FFEC59\", \"#FFA23A\", \"#74737A\"]]\n",
    "\n",
    "\n",
    "fig, (ax_hits, ax_meanE, ax_rangeE) = plt.subplots(1,3, figsize=(15, 5))\n",
    "\n",
    "c_row = 4\n",
    "for intrest_n, layer_n in enumerate(layers_of_intrest):\n",
    "    # n hits\n",
    "    n_hits_fit = wish_model.backbone.n_pts_fits[layer_n]\n",
    "    mean_n_hits = np.fromiter((n_hits_fit.get_mean(p) for p in points), dtype=float)\n",
    "    std_n_hits = np.fromiter((n_hits_fit.get_standarddev(p) for p in points), dtype=float)\n",
    "    plot_line_with_devation(ax_hits, nice_hex[c_row][intrest_n], points, mean_n_hits, std_n_hits, clip_to_zero=True, label=f\"layer {layer_n}\")\n",
    "\n",
    "    # event mean point energy\n",
    "    meanE_fit = wish_model.backbone.energy_fits[layer_n]\n",
    "    mean_meanE = np.fromiter((meanE_fit.get_mean(p) for p in points), dtype=float)\n",
    "    std_meanE = np.fromiter((meanE_fit.get_standarddev(p) for p in points), dtype=float)\n",
    "    plot_line_with_devation(ax_meanE, nice_hex[c_row][intrest_n], points, mean_meanE, std_meanE, clip_to_zero=True, label=f\"layer {layer_n}\")\n",
    "\n",
    "    # event varation in point energy\n",
    "    layer = wish_model.layers[layer_n].energy_fit\n",
    "    mean_layerE = np.ones(n_points)\n",
    "    std_layerE = np.fromiter((layer.get_standarddev(p) for p in points), dtype=float)\n",
    "    plot_line_with_devation(ax_rangeE, nice_hex[c_row][intrest_n], points, std_layerE, 0, clip_to_zero=True, label=f\"layer {layer_n}\")\n",
    "    \n",
    "    \n",
    "ax_hits.set_xlabel(\"Incedent energy\")\n",
    "ax_meanE.set_xlabel(\"Incedent energy\")\n",
    "ax_rangeE.set_xlabel(\"Incedent energy\")\n",
    "ax_hits.set_ylabel(\"Number of hits in layer\")\n",
    "ax_meanE.set_ylabel(\"Mean energy of point in layer\")\n",
    "ax_rangeE.set_ylabel(\"Standard devation of energies of points in an event\")\n",
    "    \n",
    "ax_hits.legend()\n",
    "ax_rangeE.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41ea88-e100-4687-aa81-522a18fd84b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "high_level_stats = HighLevelStats(accumulated_stats, config.poly_degree)\n",
    "hls_points = high_level_stats.incident_energy_bin_centers\n",
    "\n",
    "def get_n_pts(layer_n):\n",
    "    mean = high_level_stats.get_n_pts_vs_incident_energy(layer_n)\n",
    "    mean_sq = accumulated_stats.evt_counts_sq_hist[1:-1][high_level_stats.event_mask, layer_n]/high_level_stats.total_events_per_incedent_energy\n",
    "    std = np.sqrt(mean_sq - mean**2)\n",
    "    return mean, std\n",
    "\n",
    "def get_meanE(layer_n):\n",
    "    mean = accumulated_stats.evt_mean_E_hist[1:-1][high_level_stats.event_mask, layer_n]/high_level_stats.total_events_per_incedent_energy\n",
    "    mean_sq = accumulated_stats.evt_mean_E_sq_hist[1:-1][high_level_stats.event_mask, layer_n]/high_level_stats.total_events_per_incedent_energy\n",
    "    std = np.sqrt(mean_sq - mean**2)\n",
    "    return mean, std\n",
    "\n",
    "def get_rangeE(layer_n):\n",
    "    mean_sq = accumulated_stats.evt_mean_E_sq_hist[1:-1][high_level_stats.event_mask, layer_n]/high_level_stats.total_events_per_incedent_energy\n",
    "    mean_pnt_sq = accumulated_stats.pnt_mean_E_sq_hist[1:-1][high_level_stats.event_mask, layer_n]/high_level_stats.total_events_per_incedent_energy\n",
    "    std = np.sqrt(mean_pnt_sq - mean_sq)\n",
    "    return std\n",
    "    \n",
    "fig, (ax_hits, ax_meanE, ax_rangeE) = plt.subplots(1,3, figsize=(15, 5))\n",
    "\n",
    "for intrest_n, layer_n in enumerate(layers_of_intrest):\n",
    "    # n hits\n",
    "    mean_n_hits, std_n_hits = get_n_pts(layer_n)\n",
    "    plot_line_with_devation(ax_hits, nice_hex[c_row][intrest_n], points, mean_n_hits, std_n_hits, clip_to_zero=True, label=f\"layer {layer_n}\")\n",
    "\n",
    "    # event mean point energy\n",
    "    mean_meanE, std_meanE = get_meanE(layer_n)\n",
    "    plot_line_with_devation(ax_meanE, nice_hex[c_row][intrest_n], points, mean_meanE, std_meanE, clip_to_zero=True, label=f\"layer {layer_n}\")\n",
    "\n",
    "    # event varation in point energy\n",
    "    std_layerE = get_rangeE(layer_n)\n",
    "    plot_line_with_devation(ax_rangeE, nice_hex[c_row][intrest_n], points, std_layerE, 0, clip_to_zero=True, label=f\"layer {layer_n}\")\n",
    "    \n",
    "    \n",
    "ax_hits.set_xlabel(\"Incedent energy\")\n",
    "ax_meanE.set_xlabel(\"Incedent energy\")\n",
    "ax_rangeE.set_xlabel(\"Incedent energy\")\n",
    "ax_hits.set_ylabel(\"Number of hits in layer\")\n",
    "ax_meanE.set_ylabel(\"Mean energy of point in layer\")\n",
    "ax_rangeE.set_ylabel(\"Standard devation of energies of points in an event\")\n",
    "    \n",
    "ax_hits.legend()\n",
    "ax_rangeE.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585ec13-203c-44f7-b604-e4e035bda376",
   "metadata": {},
   "source": [
    "Generate some data from that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e888678-5d76-4a68-ba72-35c52d8b5417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "from pointcloud.utils.detector_map import split_to_layers\n",
    "\n",
    "n_tries_per_point = 10\n",
    "wish_histo_path = f\"../../../point-cloud-diffusion-data/wish_histo_data{varient}.h5\"\n",
    "\n",
    "import os, h5py\n",
    "if os.path.exists(wish_histo_path) and not redo_wish_data:\n",
    "    opened_wish_histo = h5py.File(wish_histo_path, 'r')\n",
    "    wish_observed_sum_E_means = opened_wish_histo[\"wish_observed_sum_E_means\"][:]\n",
    "    wish_observed_sum_E_stds = opened_wish_histo[\"wish_observed_sum_E_stds\"][:]\n",
    "    wish_observed_layer_E_means = opened_wish_histo[\"wish_observed_layer_E_means\"][:]\n",
    "    wish_observed_layer_E_stds = opened_wish_histo[\"wish_observed_layer_E_stds\"][:]\n",
    "    \n",
    "    wish_observed_sum_hits_means = opened_wish_histo[\"wish_observed_sum_hits_means\"][:]\n",
    "    wish_observed_sum_hits_stds = opened_wish_histo[\"wish_observed_sum_hits_stds\"][:]\n",
    "    wish_observed_layer_hits_means = opened_wish_histo[\"wish_observed_layer_hits_means\"][:]\n",
    "    wish_observed_layer_hits_stds = opened_wish_histo[\"wish_observed_layer_hits_stds\"][:]\n",
    "else:\n",
    "\n",
    "    wish_observed_sum_E_means = np.empty(n_points)\n",
    "    wish_observed_sum_E_stds = np.empty(n_points)\n",
    "    wish_observed_layer_E_means = np.empty((n_points, n_layers))\n",
    "    wish_observed_layer_E_stds = np.empty((n_points, n_layers))\n",
    "\n",
    "    wish_observed_sum_hits_means = np.empty(n_points)\n",
    "    wish_observed_sum_hits_stds = np.empty(n_points)\n",
    "    wish_observed_layer_hits_means = np.empty((n_points, n_layers))\n",
    "    wish_observed_layer_hits_stds = np.empty((n_points, n_layers))\n",
    "\n",
    "    layer_bottom_pos = np.linspace(0, 29, 30)\n",
    "\n",
    "    for i, incident in enumerate(points):\n",
    "        energies_per_layer = []\n",
    "        energies_total = []\n",
    "        hits_per_layer = []\n",
    "        hits_total = []\n",
    "        for _ in range(n_tries_per_point):\n",
    "            xs, ys, zs, es = wish_model.inference(incident)\n",
    "            energies_total.append(np.sum(es))\n",
    "            hits_total.append(len(es))\n",
    "            all_layers = np.vstack((xs, ys, zs, es)).T\n",
    "            layer_energies = []\n",
    "            layer_hits = []\n",
    "            for layer in split_to_layers(all_layers, layer_bottom_pos, 0.5):\n",
    "                layer_energies.append(np.sum(layer[:, 3]))\n",
    "                layer_hits.append(len(layer))\n",
    "            energies_per_layer.append(layer_energies)\n",
    "            hits_per_layer.append(layer_hits)\n",
    "        wish_observed_sum_E_means[i] = np.mean(energies_total)\n",
    "        wish_observed_sum_E_stds[i] = np.std(energies_total)\n",
    "        wish_observed_sum_hits_means[i] = np.mean(hits_total)\n",
    "        wish_observed_sum_hits_stds[i] = np.std(hits_total)\n",
    "        for layer_n in range(n_layers):\n",
    "            layer_energies = [e[layer_n] for e in energies_per_layer]\n",
    "            wish_observed_layer_E_means[i, layer_n] = np.mean(layer_energies)\n",
    "            wish_observed_layer_E_stds[i, layer_n] = np.std(layer_energies)\n",
    "            layer_hits = [h[layer_n] for h in hits_per_layer]\n",
    "            wish_observed_layer_hits_means[i, layer_n] = np.mean(layer_hits)\n",
    "            wish_observed_layer_hits_stds[i, layer_n] = np.std(layer_hits)\n",
    "        print(f\"{i/n_points:.0%}\", end='\\r')\n",
    "\n",
    "    # save for next time\n",
    "    opened_wish_histo = h5py.File(wish_histo_path, 'w')\n",
    "    opened_wish_histo.create_dataset(\"wish_observed_sum_E_means\", data=wish_observed_sum_E_means)\n",
    "    opened_wish_histo.create_dataset(\"wish_observed_sum_E_stds\", data=wish_observed_sum_E_stds)\n",
    "    opened_wish_histo.create_dataset(\"wish_observed_layer_E_means\", data=wish_observed_layer_E_means)\n",
    "    opened_wish_histo.create_dataset(\"wish_observed_layer_E_stds\", data=wish_observed_layer_E_stds)\n",
    "    opened_wish_histo.create_dataset(\"wish_observed_sum_hits_means\", data=wish_observed_sum_hits_means)\n",
    "    opened_wish_histo.create_dataset(\"wish_observed_sum_hits_stds\", data=wish_observed_sum_hits_stds)\n",
    "    opened_wish_histo.create_dataset(\"wish_observed_layer_hits_means\", data=wish_observed_layer_hits_means)\n",
    "    opened_wish_histo.create_dataset(\"wish_observed_layer_hits_stds\", data=wish_observed_layer_hits_stds)\n",
    "    opened_wish_histo.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f664c86-12b6-4be4-a3d0-c6ceaebe3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig, ((ax_e, ax_hit), (ax_e_ratio, ax_hit_ratio)) =\\\n",
    "    plt.subplots(2, 2, figsize=(15, 7), gridspec_kw={'height_ratios':[3, 1]})\n",
    "\n",
    "#energy\n",
    "ax_e.semilogy()\n",
    "\n",
    "global_hist_params = dict(bins=incident_bins, histtype=\"step\", lw=2)\n",
    "\n",
    "g4_hist_params1 = dict(label=\"G4\", color=nice_hex[2][0], **global_hist_params)\n",
    "ax_e.hist(points, weights=g4_observed_energies, **g4_hist_params1)\n",
    "wish_hist_params1 = dict(label=\"wish\", color=nice_hex[2][3], **global_hist_params)\n",
    "ax_e.hist(points, weights=wish_observed_sum_E_means, **wish_hist_params1)\n",
    "\n",
    "ax_e.set_xlabel(\"Incident energy\")\n",
    "ax_e.set_ylabel(\"Observed energy\")\n",
    "ax_e.legend()\n",
    "\n",
    "ax_e_ratio.plot(points, wish_observed_sum_E_means/g4_observed_energies, c=nice_hex[2][2])\n",
    "\n",
    "#hits\n",
    "ax_hit.semilogy()\n",
    "\n",
    "g4_hist_params2 = dict(label=\"G4\", color=nice_hex[2][1], **global_hist_params)\n",
    "ax_hit.hist(points, weights=g4_observed_hits, **g4_hist_params2)\n",
    "wish_hist_params2 = dict(label=\"wish\", color=nice_hex[2][4], **global_hist_params)\n",
    "ax_hit.hist(points, weights=wish_observed_sum_hits_means, **wish_hist_params2)\n",
    "\n",
    "ax_hit.set_xlabel(\"Incident energy\")\n",
    "ax_hit.set_ylabel(\"Observed hits\")\n",
    "ax_hit.legend()\n",
    "\n",
    "ax_hit_ratio.plot(points, wish_observed_sum_hits_means/g4_observed_hits, c=nice_hex[2][2])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad400cff-ff63-44af-b698-8b6a4c8d9de1",
   "metadata": {},
   "source": [
    "Note that what is being compared here is wish to the accumulated stats. It would show issues in the converstion from stats to model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8c2a2-c1e5-4375-a780-5ca36a0de2d4",
   "metadata": {},
   "source": [
    "Also, look at then energies in some layers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68271133-8bfd-4232-a301-8d731d9f90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_intrest, 2, figsize=(15, 3*n_intrest))\n",
    "\n",
    "for ax_n, layer_n in enumerate(layers_of_intrest):\n",
    "    # get the g4 data\n",
    "    g4_observed_energies_layer = np.sum(accumulated_stats.energy_hist[:, layer_n], axis=(1, 2))/non_zero_total_events\n",
    "    g4_observed_energies_layer = g4_observed_energies_layer[1:-1]\n",
    "    g4_observed_hits_layer = np.sum(accumulated_stats.counts_hist[:, layer_n], axis=(1, 2))/non_zero_total_events\n",
    "    g4_observed_hits_layer = g4_observed_hits_layer[1:-1]\n",
    "    #energy\n",
    "    ax_e = axes[ax_n, 0]\n",
    "    ax_e.semilogy()\n",
    "    \n",
    "    ax_e.hist(points, weights=g4_observed_energies_layer, **g4_hist_params1)\n",
    "    ax_e.hist(points, weights=wish_observed_layer_E_means[:, layer_n], **wish_hist_params1)\n",
    "\n",
    "    ax_e.set_xlabel(\"Incident energy\")\n",
    "    ax_e.set_ylabel(f\"Observed energy, layer {layer_n}\")\n",
    "    ax_e.legend()\n",
    "\n",
    "    #hits\n",
    "    ax_hit = axes[ax_n, 1]\n",
    "    ax_hit.semilogy()\n",
    "\n",
    "    ax_hit.hist(points, weights=g4_observed_hits_layer, **g4_hist_params2)\n",
    "    ax_hit.hist(points, weights=wish_observed_layer_hits_means[:, layer_n], **wish_hist_params2)\n",
    "\n",
    "    ax_hit.set_xlabel(\"Incident energy\")\n",
    "    ax_hit.set_ylabel(f\"Observed hits, layer {layer_n}\")\n",
    "    ax_hit.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db071204-8a8d-40dc-84cc-737d89dbb21a",
   "metadata": {},
   "source": [
    "### Evaluation of Histograms\n",
    "\n",
    "\n",
    "There is a normalisation issue in the observed energy.\n",
    "It's not that good, layer 7 looks too low everywhere for example.\n",
    "It's not disastrous either though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b39629-9350-4e3b-a80e-96281dac8467",
   "metadata": {},
   "source": [
    "## Scatter plots\n",
    "\n",
    "Start by getting some g4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3e575-4bd8-4d3b-9f80-0d8bd3dd2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import h5py\n",
    "file_path = \"/mnt/beegfs/desy/user/dayhallh/data/ILCsoftEvents/p22_th90_ph90_en10-100_joined/p22_th90_ph90_en10-100_seed42_all_steps.hdf5\"\n",
    "opened_g4 = h5py.File(file_path)\n",
    "g4_events = opened_g4['events']\n",
    "incidents = np.squeeze(opened_g4['energy'])\n",
    "n_events, max_points, _ = g4_events.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86711958-8c30-4469-b985-f1ae69d0bc89",
   "metadata": {},
   "source": [
    "Generate some equivalent data from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c265230-9059-43dd-b27b-490b1631ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "wish_file_path = f\"../../../point-cloud-diffusion-data/even_batch_10k_wish{varient}.h5\"\n",
    "\n",
    "if os.path.exists(wish_file_path) and not redo_wish_data:\n",
    "    opened_wish = h5py.File(wish_file_path, 'r')\n",
    "    wish_events = opened_wish[\"event\"][:]\n",
    "else:\n",
    "    wish_events = np.zeros_like(g4_events)\n",
    "    n_events = g4_events.shape[0]\n",
    "    max_points = g4_events.shape[1]\n",
    "    for i, incident in enumerate(incidents):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"{i/n_events:.0%}\", end=\"\\r\")\n",
    "        xs, ys, zs, es = wish_model.inference(incident)\n",
    "        n_pts = len(xs)\n",
    "        wish_events[i, :n_pts, 0] = xs[:max_points]\n",
    "        wish_events[i, :n_pts, 1] = ys[:max_points]\n",
    "        wish_events[i, :n_pts, 2] = zs[:max_points]\n",
    "        wish_events[i, :n_pts, 3] = es[:max_points]\n",
    "    # Save that\n",
    "    opened_wish = h5py.File(wish_file_path, 'w')\n",
    "    opened_wish.create_dataset(\"event\", data=wish_events)\n",
    "    opened_wish.create_dataset(\"energy\", data=incidents)\n",
    "    opened_wish.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e241892-5dd9-4ffe-9a82-b7669a9d3ab8",
   "metadata": {},
   "source": [
    "Now scatters like the things that were histogrammed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e1109-80e1-4766-958f-1a21d2d48a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sum_data = {\"incident energy\": incidents}\n",
    "\n",
    "#energy\n",
    "g4_energies = g4_events[:, :, 3]\n",
    "g4_energies[g4_energies<energy_lower] = 0.\n",
    "g4_energies[g4_energies>energy_upper] = 0.\n",
    "\n",
    "sum_data[\"g4 energy\"] = np.sum(g4_energies[:, :], axis=1)\n",
    "sum_data[\"wish energy\"] = np.sum(wish_events[:, :, 3], axis=1)\n",
    "\n",
    "# hits\n",
    "sum_data[\"g4 hits\"] = np.sum(g4_energies[:, :]>0, axis=1)\n",
    "sum_data[\"wish hits\"] = np.sum(wish_events[:, :, 3]>0, axis=1)\n",
    "\n",
    "sum_dataframe = pd.DataFrame(data=sum_data)\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2) \n",
    "g_sub_fig_e = px.scatter(sum_dataframe, x=\"incident energy\", y=[\"g4 energy\"],\n",
    "                       opacity=0.5,\n",
    "                       color_discrete_sequence=[nice_hex[2][0], nice_hex[2][3]])\n",
    "g_sub_fig_hits = px.scatter(sum_dataframe, x=\"incident energy\", y=[\"g4 hits\"],\n",
    "                          opacity=0.5,\n",
    "                          color_discrete_sequence=[nice_hex[2][1], nice_hex[2][4]])\n",
    "sub_fig_e = px.scatter(sum_dataframe, x=\"incident energy\", y=[\"g4 energy\", \"wish energy\"],\n",
    "                       opacity=0.5,\n",
    "                       color_discrete_sequence=[nice_hex[2][0], nice_hex[2][3]])\n",
    "sub_fig_hits = px.scatter(sum_dataframe, x=\"incident energy\", y=[\"g4 hits\", \"wish hits\"],\n",
    "                          opacity=0.5,\n",
    "                          color_discrete_sequence=[nice_hex[2][1], nice_hex[2][4]])\n",
    "# add each trace (or traces) to its specific subplot\n",
    "for i in g_sub_fig_e.data :\n",
    "    fig.add_trace(i, row=1, col=1)\n",
    "\n",
    "for i in g_sub_fig_hits.data :    \n",
    "    fig.add_trace(i, row=1, col=2)\n",
    "    \n",
    "for i in sub_fig_e.data :\n",
    "    fig.add_trace(i, row=2, col=1)\n",
    "\n",
    "for i in sub_fig_hits.data :    \n",
    "    fig.add_trace(i, row=2, col=2)\n",
    "fig['layout']['xaxis']['title']='Incident energy'\n",
    "fig['layout']['yaxis']['title']='Observed energy'\n",
    "fig['layout']['xaxis2']['title']='Incident energy'\n",
    "fig['layout']['yaxis2']['title']='Observed hits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398709e3-495a-4300-9f40-8fb324ffc059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de83d04-4cea-4ee0-a782-5e0e7536e416",
   "metadata": {},
   "source": [
    "Now to get the energy per layer, it's going to be easiest to define a completely new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef65f11-2dae-48f8-af56-3b18c6d872dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "g4_energy_per_layer = {f\"layer {layer_n}, g4 energy\": np.empty(n_events) for layer_n in range(n_layers)}\n",
    "g4_hits_per_layer = {f\"layer {layer_n}, g4 hits\": np.empty(n_events) for layer_n in range(n_layers)}\n",
    "\n",
    "print(\"Processing g4\")\n",
    "layer_bottom_pos = np.linspace(-1, 1, n_layers)\n",
    "cell_thickness_global = layer_bottom_pos[1] - layer_bottom_pos[0]\n",
    "for event_n, event in enumerate(g4_events):\n",
    "    for layer_n, layer in enumerate(split_to_layers(event, layer_bottom_pos, cell_thickness_global)):\n",
    "        mask = (layer[:, 3] > energy_lower) * (layer[:, 3] < energy_upper)\n",
    "        g4_energy_per_layer[f\"layer {layer_n}, g4 energy\"][event_n] = np.sum(layer[mask, 3])\n",
    "        g4_hits_per_layer[f\"layer {layer_n}, g4 hits\"][event_n] = np.sum(mask)\n",
    "\n",
    "\n",
    "wish_energy_per_layer = {f\"layer {layer_n}, wish energy\": np.empty(n_events) for layer_n in range(n_layers)}\n",
    "wish_hits_per_layer = {f\"layer {layer_n}, wish hits\": np.empty(n_events) for layer_n in range(n_layers)}\n",
    "\n",
    "print(\"Processing wish\")\n",
    "layer_bottom_pos = np.linspace(0, 29, n_layers)\n",
    "cell_thickness_global = layer_bottom_pos[1] - layer_bottom_pos[0]\n",
    "for event_n, event in enumerate(wish_events):\n",
    "    for layer_n, layer in enumerate(split_to_layers(event, layer_bottom_pos, cell_thickness_global)):\n",
    "        mask = layer[:, 3] > 0\n",
    "        wish_energy_per_layer[f\"layer {layer_n}, wish energy\"][event_n] = np.sum(layer[mask, 3])\n",
    "        wish_hits_per_layer[f\"layer {layer_n}, wish hits\"][event_n] = np.sum(mask)\n",
    "\n",
    "all_data = {\"incident energy\": incidents}\n",
    "all_data.update(g4_energy_per_layer)\n",
    "all_data.update(g4_hits_per_layer)\n",
    "all_data.update(wish_energy_per_layer)\n",
    "all_data.update(wish_hits_per_layer)\n",
    "\n",
    "per_layer = pd.DataFrame(data=all_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a48d92-c509-4064-8532-a7c94a974a8a",
   "metadata": {},
   "source": [
    "### Layer by layer comparisons.\n",
    "\n",
    "Now the same thing, but split by layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c31ad-47cf-4123-a6f8-b6ccedc2ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_fig = make_subplots(rows=len(layers_of_intrest), cols=2) \n",
    "\n",
    "for ax_n, layer_n in enumerate(layers_of_intrest):\n",
    "    y_ax_e = [f\"layer {layer_n}, g4 energy\", f\"layer {layer_n}, wish energy\"]\n",
    "    sub_fig_e = px.scatter(per_layer, x=\"incident energy\", y=y_ax_e,\n",
    "                           opacity=0.5,\n",
    "                           color_discrete_sequence=[nice_hex[ax_n][0], nice_hex[ax_n][3]])\n",
    "    y_ax_hits = [f\"layer {layer_n}, g4 hits\", f\"layer {layer_n}, wish hits\"]\n",
    "    sub_fig_hits = px.scatter(per_layer, x=\"incident energy\", y=y_ax_hits,\n",
    "                              opacity=0.5,\n",
    "                              color_discrete_sequence=[nice_hex[ax_n][1], nice_hex[ax_n][4]])\n",
    "    # add each trace (or traces) to its specific subplot\n",
    "    for i in sub_fig_e.data :\n",
    "        layer_fig.add_trace(i, row=ax_n+1, col=1)\n",
    "    \n",
    "    for i in sub_fig_hits.data :    \n",
    "        layer_fig.add_trace(i, row=ax_n+1, col=2)\n",
    "    e_ax_num = (ax_n)*2 + 1\n",
    "    e_ax_num = (ax_n)*2 + 1\n",
    "    layer_fig['layout'][f'xaxis{e_ax_num}']['title'] = \"Incident energy\"\n",
    "    layer_fig['layout'][f'yaxis{e_ax_num}']['title'] = \"Observed energy\"\n",
    "    hit_ax_num = e_ax_num + 1\n",
    "    layer_fig['layout'][f'xaxis{hit_ax_num}']['title'] = \"Incident energy\"\n",
    "    layer_fig['layout'][f'yaxis{hit_ax_num}']['title'] = \"Observed hits\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32449e-a40d-41a7-865f-87c2e4571870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "layer_fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=1200,\n",
    ")\n",
    "\n",
    "\n",
    "layer_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc292e-f4ec-4ca6-92bc-68fcebc5074b",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "The energy is further off than the hits. Some layers are worse than others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0280ad8a-3c13-4328-86e4-b53ae761dbbc",
   "metadata": {},
   "source": [
    "## Fixed energy slices\n",
    "\n",
    "Now we will sample and make fixed energy slices of data. We will save the produced files to speed up re-running. Start by defining the boundaries of each slice.\n",
    "\n",
    "In this part, we won't bother to split g4 into energy boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471544e-56e4-4cda-8050-4177a7208752",
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_min = 10\n",
    "incident_max = 90\n",
    "n_energy_slices = 5\n",
    "energy_slice_width = 1\n",
    "slice_lower_bounds = np.linspace(incident_min, incident_max-energy_slice_width, n_energy_slices).astype(int)\n",
    "max_showers_in_slice = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d5b5b8-370d-4ade-8460-27c3c9ccca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "g4_slice_paths = \"../../../point-cloud-diffusion-data/energy_slices/g4_energy_slice_{}.h5\"\n",
    "try:\n",
    "    os.mkdir(os.path.dirname(g4_slice_paths))\n",
    "except FileExistsError:\n",
    "    pass\n",
    "from pointcloud.data.read_write import read_raw_regaxes\n",
    "config.storage_base = \"/mnt/beegfs/desy/user/\"\n",
    "import os, h5py\n",
    "g4_slice_incidents = []\n",
    "g4_slice_showers = []\n",
    "for lower in slice_lower_bounds:\n",
    "    this_slice_path = g4_slice_paths.format(lower)\n",
    "    if os.path.exists(this_slice_path) and not redo_g4_data:\n",
    "        with h5py.File(this_slice_path, 'r') as opened_slice:\n",
    "            g4_slice_incidents.append(opened_slice['energy'][:])\n",
    "            g4_slice_showers.append(opened_slice['events'][:])\n",
    "        \n",
    "    else:\n",
    "        print(f\"Skimming data for slice {lower}\")\n",
    "        data_path = config.dataset_path\n",
    "        with h5py.File(data_path, 'r') as opened_data:\n",
    "            all_incidents = opened_data['energy']\n",
    "            upper = lower + energy_slice_width\n",
    "            good_idxs = np.where(np.squeeze((all_incidents>lower) & (all_incidents<upper)))[0]\n",
    "            print(f\"found {len(good_idxs)} points for slice, will trim to {max_showers_in_slice}\")\n",
    "            good_idxs = np.random.choice(good_idxs, max_showers_in_slice, False)\n",
    "            good_idxs.sort()\n",
    "            incidents_here, showers_here = read_raw_regaxes(config, good_idxs)\n",
    "            g4_slice_incidents.append(incidents_here)\n",
    "            g4_slice_showers.append(showers_here)\n",
    "        with h5py.File(this_slice_path, 'w') as opened_slice:\n",
    "            opened_slice.create_dataset(\"energy\", data=incidents_here)\n",
    "            opened_slice.create_dataset(\"events\", data=showers_here)\n",
    "            \n",
    "            \n",
    "        \n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc1474-d06f-4712-a69d-6f2a5c38a645",
   "metadata": {},
   "source": [
    "Now we need to generate the slices from wish. This is simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27faae72-0bf5-463a-b038-aab44c96a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wish_slice_paths = \"../../../point-cloud-diffusion-data/energy_slices/wish_energy_slice_{}\" + varient + \".h5\"\n",
    "try:\n",
    "    os.mkdir(os.path.dirname(wish_slice_paths))\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "import os, h5py\n",
    "wish_slice_incidents = []\n",
    "wish_slice_showers = []\n",
    "for slice_n, lower in enumerate(slice_lower_bounds):\n",
    "    this_slice_path = wish_slice_paths.format(lower)\n",
    "    if os.path.exists(this_slice_path) and not redo_wish_data:\n",
    "        with h5py.File(this_slice_path, 'r') as opened_slice:\n",
    "            wish_slice_incidents.append(opened_slice['energy'][:])\n",
    "            wish_slice_showers.append(opened_slice['events'][:])\n",
    "        \n",
    "    else:\n",
    "        print(f\"Generating data for slice {lower}\")\n",
    "        #incidents_here = np.linspace(lower, lower+energy_slice_width, max_showers_in_slice)\n",
    "        incidents_here = g4_slice_incidents[slice_n]\n",
    "        showers_here = np.zeros((max_showers_in_slice, max_points, 4))\n",
    "        for shower_n, incident_e in enumerate(incidents_here):\n",
    "            xs, ys, zs, es = wish_model.inference(incident_e)\n",
    "            n_pts = len(xs)\n",
    "            showers_here[shower_n, :n_pts, 0] = xs[:max_points]\n",
    "            showers_here[shower_n, :n_pts, 1] = ys[:max_points]\n",
    "            showers_here[shower_n, :n_pts, 2] = zs[:max_points]\n",
    "            showers_here[shower_n, :n_pts, 3] = es[:max_points]\n",
    "        wish_slice_incidents.append(incidents_here)\n",
    "        wish_slice_showers.append(showers_here)\n",
    "        with h5py.File(this_slice_path, 'w') as opened_slice:\n",
    "            opened_slice.create_dataset(\"energy\", data=incidents_here)\n",
    "            opened_slice.create_dataset(\"events\", data=showers_here)\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28233583-0a8c-42e4-88e4-98059b40ea39",
   "metadata": {},
   "source": [
    "## Plotting energy slices\n",
    "We break this down by both energy slice, and layer number. This gives us a grid, with each column repereseting energy slice, and each row reperesenting a layer number. A busy plot, as such the histograms for n_hits will be shown first, then the histograms for observed energy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f60fb-fa70-4fbf-b29e-e50b9942ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "g4_slices_nhits = np.empty((len(layers_of_intrest), n_energy_slices, max_showers_in_slice))\n",
    "g4_slices_mean_energy = np.zeros_like(g4_slices_nhits)\n",
    "g4_slices_point_energy = [[[] for _ in range(n_energy_slices)] for _ in layers_of_intrest]\n",
    "\n",
    "g4_up_slices_nhits = np.empty((len(layers_of_intrest), n_energy_slices, max_showers_in_slice))\n",
    "g4_up_slices_mean_energy = np.zeros_like(g4_slices_nhits)\n",
    "g4_up_slices_point_energy = [[[] for _ in range(n_energy_slices)] for _ in layers_of_intrest]\n",
    "\n",
    "g4_down_slices_nhits = np.empty((len(layers_of_intrest), n_energy_slices, max_showers_in_slice))\n",
    "g4_down_slices_mean_energy = np.zeros_like(g4_slices_nhits)\n",
    "g4_down_slices_point_energy = [[[] for _ in range(n_energy_slices)] for _ in layers_of_intrest]\n",
    "cut = 0.00006\n",
    "\n",
    "print(\"Processing g4\")\n",
    "layer_bottom_pos = np.linspace(0, 29, n_layers)\n",
    "cell_thickness_global = layer_bottom_pos[1] - layer_bottom_pos[0]\n",
    "for energy_slice_n in range(n_energy_slices):\n",
    "    for event_n, event in enumerate(g4_slice_showers[energy_slice_n]):\n",
    "        intrest_reached = 0\n",
    "        for layer_n, layer in enumerate(split_to_layers(event, layer_bottom_pos, cell_thickness_global)):\n",
    "            if layer_n not in layers_of_intrest:\n",
    "                continue\n",
    "            mask = layer[:, 3] > 0 \n",
    "            \n",
    "            g4_slices_nhits[intrest_reached][energy_slice_n][event_n] = np.sum(mask)\n",
    "            \n",
    "            up_mask = layer[:, 3] > cut\n",
    "            down_mask = (layer[:, 3] > 0) * (layer[:, 3] < cut) \n",
    "\n",
    "            \n",
    "            g4_up_slices_nhits[intrest_reached][energy_slice_n][event_n] = np.sum(up_mask)\n",
    "            g4_down_slices_nhits[intrest_reached][energy_slice_n][event_n] = np.sum(down_mask)\n",
    "            \n",
    "            if np.any(mask):\n",
    "                energy_in_layer = layer[mask, 3]*PointCloudDataset.energy_scale\n",
    "                g4_slices_mean_energy[intrest_reached][energy_slice_n][event_n] = np.mean(energy_in_layer)\n",
    "                g4_slices_point_energy[intrest_reached][energy_slice_n] += energy_in_layer.tolist()\n",
    "                \n",
    "                up_energy_in_layer = layer[up_mask, 3]*PointCloudDataset.energy_scale\n",
    "                g4_up_slices_mean_energy[intrest_reached][energy_slice_n][event_n] = np.mean(up_energy_in_layer)\n",
    "                g4_up_slices_point_energy[intrest_reached][energy_slice_n] += up_energy_in_layer.tolist()\n",
    "                down_energy_in_layer = layer[down_mask, 3]*PointCloudDataset.energy_scale\n",
    "                g4_down_slices_mean_energy[intrest_reached][energy_slice_n][event_n] = np.mean(down_energy_in_layer)\n",
    "                g4_down_slices_point_energy[intrest_reached][energy_slice_n] += down_energy_in_layer.tolist()\n",
    "                \n",
    "            intrest_reached += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39b97d-d8ee-4796-a34c-040dd85b8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wish_slices_nhits = np.empty((len(layers_of_intrest), n_energy_slices, max_showers_in_slice))\n",
    "wish_slices_mean_energy = np.zeros_like(wish_slices_nhits)\n",
    "wish_slices_point_energy = [[[] for _ in range(n_energy_slices)] for _ in layers_of_intrest]\n",
    "\n",
    "print(\"Processing wish\")\n",
    "layer_bottom_pos = np.linspace(0, 29, n_layers)-0.01\n",
    "cell_thickness_global = layer_bottom_pos[1] - layer_bottom_pos[0]\n",
    "for energy_slice_n in range(n_energy_slices):\n",
    "    for event_n, event in enumerate(wish_slice_showers[energy_slice_n]):\n",
    "        intrest_reached = 0\n",
    "        for layer_n, layer in enumerate(split_to_layers(event, layer_bottom_pos, cell_thickness_global)):\n",
    "            if layer_n not in layers_of_intrest:\n",
    "                continue\n",
    "            mask = layer[:, 3] > 0\n",
    "            wish_slices_nhits[intrest_reached][energy_slice_n][event_n] = np.sum(mask)\n",
    "            if np.any(mask):\n",
    "                wish_slices_mean_energy[intrest_reached][energy_slice_n][event_n] = np.mean(layer[mask, 3])\n",
    "                wish_slices_point_energy[intrest_reached][energy_slice_n] += (layer[mask, 3]).tolist()\n",
    "            intrest_reached += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dbb113-37c9-482f-969f-b78b8ecbf407",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_hits, axarr_hits = plt.subplots(len(layers_of_intrest), n_energy_slices, sharey=True, sharex=True, figsize=(15, 12))\n",
    "high_level_stats = HighLevelStats(accumulated_stats, config.poly_degree)\n",
    "\n",
    "# which dimensions should be logs?\n",
    "log_x = False\n",
    "log_y = False\n",
    "\n",
    "\n",
    "max_hits = int(max(g4_slices_nhits.max(), wish_slices_nhits.max()))\n",
    "if log_x:\n",
    "    bins = np.logspace(np.log10(1.), np.log10(max_hits), 50)\n",
    "else:\n",
    "    bins = np.linspace(0, 1000, min(50, max_hits))\n",
    "global_hist_params = dict(bins=bins, histtype=\"step\", lw=2, density=True)\n",
    "g4_hist_params = dict(label=\"G4\", color=nice_hex[2][0], **global_hist_params)\n",
    "wish_hist_params = dict(label=\"wish\", color=nice_hex[2][3], **global_hist_params)\n",
    "up_g4_hist_params = dict(label=\"up g4\", color=nice_hex[3][0], **global_hist_params)\n",
    "down_g4_hist_params = dict(label=\"down g4\", color=nice_hex[3][3], **global_hist_params)\n",
    "\n",
    "from scipy.stats import lognorm\n",
    "stats_params = dict(lw=2, c=\"black\", label=\"High level stats\", alpha=0.5)\n",
    "stats_params2 = dict(lw=2, c=\"green\", label=\"Raw stats\", alpha=0.8)\n",
    "\n",
    "for energy_slice_n in range(n_energy_slices):\n",
    "    for intrest_n, layer_n in enumerate(layers_of_intrest):\n",
    "        ax = axarr_hits[intrest_n][energy_slice_n]\n",
    "        if intrest_n == 0:\n",
    "            ax.set_title(f\"Incident {slice_lower_bounds[energy_slice_n]}\")\n",
    "        if intrest_n == len(layers_of_intrest) -1:\n",
    "            ax.set_xlabel(\"N hits\")\n",
    "        if energy_slice_n == 0:\n",
    "            ax.set_ylabel(f\"Layer {layer_n}\")\n",
    "        mean_g4 = np.mean(g4_slices_nhits[intrest_n, energy_slice_n])\n",
    "        ax.hist(wish_slices_nhits[intrest_n, energy_slice_n], **wish_hist_params)\n",
    "        ax.hist(g4_slices_nhits[intrest_n, energy_slice_n], **g4_hist_params)\n",
    "        #ax.hist(g4_down_slices_nhits[intrest_n, energy_slice_n], **down_g4_hist_params)\n",
    "        #ax.hist(g4_up_slices_nhits[intrest_n, energy_slice_n], **up_g4_hist_params)\n",
    "        if log_x:\n",
    "            ax.semilogx()\n",
    "        if log_y:\n",
    "            ax.semilogy()\n",
    "            \n",
    "        # calculate theory from stats and add\n",
    "        grad, intercept = high_level_stats.n_pts(layer_n)[:2]\n",
    "        slice_energy = slice_lower_bounds[energy_slice_n] + 0.5*energy_slice_width\n",
    "        mu = grad*slice_energy + intercept\n",
    "        #ax.vlines(mu, 0, 1, color=\"\")\n",
    "        \n",
    "        #pmf = lognorm.pmf(k=bins.astype(int), mu=mu)\n",
    "        #ax.plot(bins, pmf, **stats_params)\n",
    "\n",
    "        # look at the \"raw\" stats\n",
    "        incident_in_hls = high_level_stats.incident_energy_bin_centers\n",
    "        closest_bin = np.argmin(np.abs(incident_in_hls-slice_energy))\n",
    "        mu = high_level_stats.get_n_pts_vs_incident_energy(layer_n)[closest_bin]\n",
    "        #ax.vlines(mu, 0, 1, color=\"red\")\n",
    "        #ax.vlines(mean_g4, 0, 1, color=\"green\")\n",
    "        #pmf = poisson.pmf(k=bins.astype(int), mu=mu)\n",
    "        #ax.plot(bins, pmf, **stats_params2)\n",
    "\n",
    "        if log_y:\n",
    "            ax.set_ylim(8e-5, 0.16)\n",
    "        else:\n",
    "            ax.set_ylim(0, 0.015)\n",
    "\n",
    "axarr_hits[0][-1].legend()\n",
    "plt.tight_layout()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779c7b4-2bc0-41e6-a25f-056fee4c2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a doodled pdf function\n",
    "from torch.distributions import LogNormal\n",
    "# to get the scale and the loc from a mean and a std\n",
    "from pointcloud.models.wish import PolyFit1DLogNormal\n",
    "\n",
    "def doodle_pdf(bins, energy_slice_n, layer_n, intercept_scale, grad_scale, intercept_shift, grad_shift):\n",
    "    y0 = 10\n",
    "    x0 = 0.001\n",
    "    y1 = 1\n",
    "    x1 = 0.01\n",
    "    m = np.log(y1/y0)/np.log(x1/x0)\n",
    "    b = y0/x0**m\n",
    "    process_1_pdf = b*(bins**m)\n",
    "    \n",
    "    slice_energy = slice_lower_bounds[energy_slice_n] + 0.5*energy_slice_width\n",
    "    mean_energy_coeffs = high_level_stats.event_mean_point_energy(layer_n)\n",
    "    mean_energy_coeffs[-1] *= intercept_scale\n",
    "    mean_energy_coeffs[-2] *= grad_scale\n",
    "    mean_energy_coeffs[-1] += intercept_shift\n",
    "    mean_energy_coeffs[-2] += grad_shift\n",
    "    stdmean_energy_coeffs = high_level_stats.stddev_event_mean_point_energy(layer_n)\n",
    "    linearFit = PolyFit1DLogNormal(torch.from_numpy(mean_energy_coeffs), torch.from_numpy(stdmean_energy_coeffs))\n",
    "    positions = torch.Tensor(bins)\n",
    "    process_2_pdf = np.exp(linearFit.calculate_log_probability(slice_energy, positions).numpy())\n",
    "\n",
    "    return process_1_pdf, process_2_pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4da1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_level_stats = HighLevelStats(accumulated_stats, config.poly_degree)\n",
    "\n",
    "max_means = max(np.nanmax(g4_slices_mean_energy), np.nanmax(wish_slices_mean_energy))\n",
    "\n",
    "log_x = False\n",
    "log_y = True\n",
    "for log_x in [True, False]:\n",
    "    fig_means, axarr_means = plt.subplots(len(layers_of_intrest), n_energy_slices, sharey=True, sharex=True, figsize=(15, 12))\n",
    "    if log_x:\n",
    "        bins = np.logspace(np.log10(0.001), np.log10(max_means), 50)\n",
    "    else:\n",
    "        #bins = np.linspace(0, max_means, 50)\n",
    "        bins = np.linspace(0.001, 1, 50)\n",
    "        #bins = np.linspace(0, 0.2, 50)\n",
    "    global_hist_params = dict(bins=bins, histtype=\"step\", lw=2, density=True)\n",
    "    g4_hist_params = dict(label=\"G4\", color=nice_hex[3][0], **global_hist_params)\n",
    "    up_g4_hist_params = dict(label=\"up G4\", color=nice_hex[1][0], **global_hist_params)\n",
    "    down_g4_hist_params = dict(label=\"down G4\", color=nice_hex[4][3], **global_hist_params)\n",
    "    wish_hist_params = dict(label=\"wish\", color=nice_hex[3][3], **global_hist_params)\n",
    "    \n",
    "    #from scipy.stats import ??\n",
    "    stats_params = dict(lw=2, c=\"black\", label=\"High level stats\", alpha=0.5)\n",
    "    stats_params2 = dict(lw=2, c=\"green\", label=\"Raw stats\", alpha=0.8)\n",
    "    \n",
    "    for energy_slice_n in range(n_energy_slices):\n",
    "        for intrest_n, layer_n in enumerate(layers_of_intrest):\n",
    "            ax = axarr_means[intrest_n][energy_slice_n]\n",
    "            if intrest_n == 0:\n",
    "                ax.set_title(f\"Incident {slice_lower_bounds[energy_slice_n]}\")\n",
    "            if intrest_n == len(layers_of_intrest) -1:\n",
    "                ax.set_xlabel(\"Mean event energy\")\n",
    "            if energy_slice_n == 0:\n",
    "                ax.set_ylabel(f\"Layer {layer_n}\")\n",
    "            ax.hist(wish_slices_mean_energy[intrest_n, energy_slice_n], **wish_hist_params)\n",
    "            ax.hist(g4_slices_mean_energy[intrest_n, energy_slice_n], **g4_hist_params)\n",
    "            #ax.hist(g4_down_slices_mean_energy[intrest_n, energy_slice_n], **down_g4_hist_params)\n",
    "            #ax.hist(g4_up_slices_mean_energy[intrest_n, energy_slice_n], **up_g4_hist_params)\n",
    "            if log_y:\n",
    "                ax.semilogy()\n",
    "            if log_x:\n",
    "                ax.semilogx()\n",
    "                \n",
    "    \n",
    "            # doodle another fit\n",
    "            #process_1_pdf, process_2_pdf = doodle_pdf(bins, energy_slice_n, layer_n, 1., 0.5, 0.1, 0.)\n",
    "        \n",
    "            #ax.plot(bins, process_1_pdf, c=\"orange\", linestyle='--')\n",
    "            #ax.plot(bins, process_1_pdf + process_2_pdf, c=\"orange\", linestyle='-.')\n",
    "            if not log_y:\n",
    "                ax.set_ylim(0., 20)\n",
    "            else:\n",
    "                ax.set_ylim(8e-2, 100.)\n",
    "    \n",
    "    axarr_means[0][-1].legend()\n",
    "    plt.tight_layout()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817060a6-7aea-44d1-9324-259ef52be207",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_energy, axarr_energy = plt.subplots(len(layers_of_intrest), n_energy_slices, sharey=True, sharex=True, figsize=(15, 12))\n",
    "\n",
    "max_energy = max(np.max([np.max([np.max(ps) for ps in layer]) for layer in g4_slices_point_energy]),\n",
    "                 np.max([np.max([np.max(ps) for ps in layer]) for layer in wish_slices_point_energy]))\n",
    "\n",
    "log_x = False\n",
    "log_y = True\n",
    "\n",
    "if log_x:\n",
    "    bins = np.logspace(np.log10(1.), np.log10(75), 50)\n",
    "else:\n",
    "    bins = np.linspace(0, 10., 35)\n",
    "\n",
    "global_hist_params = dict(bins=bins, histtype=\"step\", lw=2, density=True)\n",
    "g4_hist_params = dict(label=\"G4\", color=nice_hex[2][1], **global_hist_params)\n",
    "up_g4_hist_params = dict(label=\"up G4\", color=nice_hex[0][1], **global_hist_params)\n",
    "down_g4_hist_params = dict(label=\"down G4\", color=nice_hex[0][3], **global_hist_params)\n",
    "wish_hist_params = dict(label=\"wish\", color=nice_hex[2][4], **global_hist_params)\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "for energy_slice_n in range(n_energy_slices):\n",
    "    for intrest_n, layer_n in enumerate(layers_of_intrest):\n",
    "        ax = axarr_energy[intrest_n][energy_slice_n]\n",
    "        if intrest_n == 0:\n",
    "            ax.set_title(f\"Incident {slice_lower_bounds[energy_slice_n]}\")\n",
    "        if intrest_n == len(layers_of_intrest) -1:\n",
    "            ax.set_xlabel(\"Energy of hits\")\n",
    "        if energy_slice_n == 0:\n",
    "            ax.set_ylabel(f\"Layer {layer_n}\")\n",
    "        ax.hist(wish_slices_point_energy[intrest_n][energy_slice_n], **wish_hist_params)\n",
    "        ax.hist(g4_slices_point_energy[intrest_n][energy_slice_n], **g4_hist_params)\n",
    "        #ax.hist(g4_up_slices_point_energy[intrest_n][energy_slice_n], **up_g4_hist_params)\n",
    "        #ax.hist(g4_down_slices_point_energy[intrest_n][energy_slice_n], **down_g4_hist_params)\n",
    "        if log_y:\n",
    "            ax.semilogy()\n",
    "        if log_x:\n",
    "            ax.semilogx()\n",
    "        \n",
    "        # calculate theory from stats and add\n",
    "        #mean_energy_grad, mean_energy_intercept = high_level_stats.event_mean_point_energy(layer_n)\n",
    "        #slice_energy = slice_lower_bounds[energy_slice_n] + 0.5*energy_slice_width\n",
    "        #mean_energy = mean_energy_grad*slice_energy + mean_energy_intercept\n",
    "        #stdmean_energy_grad, stdmean_energy_intercept = high_level_stats.stddev_event_mean_point_energy(layer_n)\n",
    "        #slice_energy = slice_lower_bounds[energy_slice_n] + 0.5*energy_slice_width\n",
    "        #stdmean_energy = stdmean_energy_grad*slice_energy + stdmean_energy_intercept\n",
    "        #stdpoint_energy_grad, stdpoint_energy_intercept = high_level_stats.stddev_point_energy_in_evt(layer_n)\n",
    "        #slice_energy = slice_lower_bounds[energy_slice_n] + 0.5*energy_slice_width\n",
    "        #stdpoint_energy = stdpoint_energy_grad*slice_energy + stdpoint_energy_intercept\n",
    "        #std_energy = stdmean_energy + stdpoint_energy*mean_energy\n",
    "        #pdf = norm.pdf(bins, loc=mean_energy, scale=std_energy)\n",
    "        #ax.plot(bins, pdf, **stats_params)\n",
    "        \n",
    "        #ax.set_ylim(2e-6, 1)\n",
    "\n",
    "axarr_energy[0][-1].legend()\n",
    "plt.tight_layout()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d2fd3a-e347-4f51-9163-3671a5297fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calogpu",
   "language": "python",
   "name": "calogpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
