{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5962ffa2-a095-4d3b-bc57-e53517d97477",
   "metadata": {},
   "source": [
    "# ShowerAlign\n",
    "\n",
    "Plotting the output of AlignedStatsAccumulators.\n",
    "\n",
    "Let us imagine that we find the right distribution from the shift, then we know how to generate the correct center points for the shower.\n",
    "The number of hits found at a given point along the shower axis could be a function of;\n",
    "\n",
    "- The incident energy\n",
    "- The depth of the shower (size of the shift)\n",
    "- The distance from the point to the center of the shower along the shower axis.\n",
    "\n",
    "We will start by looking at the function that defines the layer offset itself.\n",
    "\n",
    "- Is this corrilated with incident energy?\n",
    "- What is the distribution in a slice?\n",
    "\n",
    "This must be done with raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ace40f-18f2-4ce1-a48d-9285ac9dd283",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/home/henry/training/point-cloud-diffusion-logs/wish/dataset_accumulators/p22_th90_ph90_en10-1/p22_th90_ph90_en10-100_seedAll_alignMeanEven.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m align_odd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malign\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malign_center\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mOdd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m align_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malign\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malign_center\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m acc_even \u001b[38;5;241m=\u001b[39m \u001b[43mAlignedStatsAccumulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_even\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m acc_odd \u001b[38;5;241m=\u001b[39m AlignedStatsAccumulator\u001b[38;5;241m.\u001b[39mload(save_location(config, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, align_odd))\n\u001b[1;32m     26\u001b[0m acc_all \u001b[38;5;241m=\u001b[39m AlignedStatsAccumulator\u001b[38;5;241m.\u001b[39mload(save_location(config, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, align_all))\n",
      "File \u001b[0;32m~/training/point-cloud-diffusion/scripts/plotting/pointcloud/utils/stats_accumulator.py:660\u001b[0m, in \u001b[0;36mAlignedStatsAccumulator.load\u001b[0;34m(cls, path, varient)\u001b[0m\n\u001b[1;32m    658\u001b[0m     varient \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    659\u001b[0m shift_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m varient\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeak\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshift_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/training/point-cloud-diffusion/scripts/plotting/pointcloud/utils/stats_accumulator.py:365\u001b[0m, in \u001b[0;36mStatsAccumulator.load\u001b[0;34m(cls, path, **additional_kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madditional_kwargs):\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    Alternative constructor to load an accumulator from a file.\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m        The loaded accumulator\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;66;03m# attrs are all arguments to the constructor\u001b[39;00m\n\u001b[1;32m    367\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {attr: f\u001b[38;5;241m.\u001b[39mattrs[attr] \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_saved_attrs()}\n\u001b[1;32m    368\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mupdate(additional_kwargs)\n",
      "File \u001b[0;32m~/Programs/miniconda3/envs/caloclouds/lib/python3.11/site-packages/h5py/_hl/files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/Programs/miniconda3/envs/caloclouds/lib/python3.11/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/home/henry/training/point-cloud-diffusion-logs/wish/dataset_accumulators/p22_th90_ph90_en10-1/p22_th90_ph90_en10-100_seedAll_alignMeanEven.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from pointcloud.config_varients.wish import Configs\n",
    "from pointcloud.config_varients.wish_maxwell import Configs as MWConfigs\n",
    "from pointcloud.utils.stats_accumulator import AlignedStatsAccumulator, save_location\n",
    "from pointcloud.data.dataset import dataset_class_from_config\n",
    "from pointcloud.data.read_write import read_raw_regaxes, get_n_events\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly import express as ptx\n",
    "import pandas\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Gumbel, Exponential\n",
    "from pointcloud.utils.optimisers import curve_fit\n",
    "config = MWConfigs()\n",
    "config.dataset_path = \"/home/dayhallh/Data/p22_th90_ph90_en10-100_joined/p22_th90_ph90_en10-100_seed{}_all_steps.hdf5\"\n",
    "config.dataset_path = \"/beegfs/desy/user/dayhallh/data/ILCsoftEvents/p22_th90_ph90_en10-100_joined/p22_th90_ph90_en10-100_seed{}_all_steps.hdf5\"\n",
    "\n",
    "dataset_class = dataset_class_from_config(config)\n",
    "\n",
    "align_center = \"Mean\" if True else \"Peak\"\n",
    "align_even = f\"align{align_center}Even\"\n",
    "align_odd = f\"align{align_center}Odd\"\n",
    "align_all = f\"align{align_center}\"\n",
    "\n",
    "acc_even = AlignedStatsAccumulator.load(save_location(config, 1, 0, align_even))\n",
    "acc_odd = AlignedStatsAccumulator.load(save_location(config, 1, 0, align_odd))\n",
    "acc_all = AlignedStatsAccumulator.load(save_location(config, 1, 0, align_all))\n",
    "acc = acc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1753c0e6-55e8-443d-93df-77c442555f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_events = get_n_events(config.dataset_path, config.n_dataset_files)\n",
    "check_events = min(sum(total_events), 10_000)\n",
    "batch_size = 1_000\n",
    "shifts = np.empty((check_events, 2))\n",
    "for start_idx in range(0, check_events, batch_size):\n",
    "    print(f\"{start_idx/check_events:.0%}\", end='\\r')\n",
    "    end_idx = start_idx + batch_size\n",
    "    energies, events = read_raw_regaxes(config, pick_events=slice(start_idx, end_idx))\n",
    "    dataset_class.normalize_xyze(events)\n",
    "    shifts[start_idx:end_idx, 0] = energies\n",
    "    shifts[start_idx:end_idx, 1] = acc.get_shift(events).flatten()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ea27c-2eb0-4f58-94b2-d42c4e9647e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.DataFrame({\"incident energy\":shifts[:, 0], \"shift\":shifts[:, 1]})\n",
    "fig = ptx.scatter(dataframe, x=\"incident energy\", y=\"shift\", opacity=0.3)\n",
    "coeffs = np.polyfit(shifts[:, 0], shifts[:, 1], config.poly_degree)\n",
    "xs = np.linspace(np.min(shifts[:, 0]), np.max(shifts[:, 0]), 100)\n",
    "ys = np.polyval(coeffs, xs)\n",
    "fig.add_scatter(x=xs, y=ys, name=\"Mean fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239dd8af-0311-4312-b51f-521faa544bda",
   "metadata": {},
   "source": [
    "This has a clear trend with incident energy, so we fit this as a polynomial, and restack the shifts.\n",
    "The objective is to see a distibution of the shifts about the mean at a given incident energy.\n",
    "\n",
    "Upon stacking these, it seems that a Gumbel distribution could be apropreate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c411a0-5bfe-4e4f-bdc6-cd12452b0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_shifts = np.copy(shifts)\n",
    "mean_at = np.polyval(coeffs, stacked_shifts[:, 0])\n",
    "stacked_shifts[:, 1] -= mean_at\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "for ax in axes:\n",
    "    heights, bins, patches = ax.hist(stacked_shifts[:, 1], bins=50, histtype=\"step\", density=True, label=\"measured\")\n",
    "\n",
    "bin_centers = 0.5*(bins[1:] + bins[:-1])\n",
    "mean = np.sum(bin_centers*heights)/np.sum(heights)\n",
    "non_zero_weights = np.sum(heights>0)\n",
    "varience = (non_zero_weights/(non_zero_weights-1))*np.sum(heights*(bin_centers-mean)**2)/np.sum(heights)\n",
    "\n",
    "beta = np.sqrt(6*varience)/np.pi\n",
    "mu = mean - beta*np.euler_gamma\n",
    "\n",
    "distribution = Gumbel(mu, beta)\n",
    "probs = np.exp(distribution.log_prob(torch.Tensor(bins)))\n",
    "for ax in axes:\n",
    "    ax.plot(bins, probs, label=\"Gumbel\")\n",
    "    ax.set_xlabel(\"aligned shift\")\n",
    "\n",
    "axes[1].semilogy()\n",
    "axes[0].legend()\n",
    "\n",
    "axes[0].set_ylabel(\"counts\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f2ce6-4191-4f6d-af25-e00d0710baaf",
   "metadata": {},
   "source": [
    "This will be used to pick shifts. And given the shift, we want to predict the number of points at any given position along the axis.\n",
    "So, three possible input values;\n",
    "\n",
    "- Incident energy\n",
    "- Shift\n",
    "- Distance from center\n",
    "\n",
    "Let's look for correlations with these.\n",
    "To start with we will check that the shifted accumulator has done something sensible.\n",
    "Pull a small sample from the data and compare.\n",
    "\n",
    "Then we will move on to looking at how tightly corrilated the three \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb3681-66c9-4d6f-82ce-c480d2e1d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ze_sample = np.zeros((check_events, config.max_points*10, 2))\n",
    "\n",
    "for start_idx in range(0, check_events, batch_size):\n",
    "    print(f\"{start_idx/check_events:.0%}\", end='\\r')\n",
    "    end_idx = start_idx + batch_size\n",
    "    energies, events = read_raw_regaxes(config, pick_events=slice(start_idx, end_idx))\n",
    "    dataset_class.normalize_xyze(events)\n",
    "    e = events[:, :, 3]\n",
    "    n_pts = e.shape[1]\n",
    "    mask = e>0\n",
    "    ze_sample[start_idx:end_idx, :n_pts, 0][mask] = events[..., 2][mask]\n",
    "    ze_sample[start_idx:end_idx, :n_pts, 1][mask] = e[mask]\n",
    "ze_sample[..., 0] -= shifts[:, 1, None]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec54ba-591f-403d-b747-b86a4e44e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "counts = acc_even.counts_hist[1:-1] + acc_odd.counts_hist[1:-1]\n",
    "counts_along_axis = np.sum(counts, axis=(2, 3))\n",
    "y_values = acc.layer_bottom\n",
    "incident_bins = acc.incident_bin_boundaries\n",
    "inci_values = 0.5*(incident_bins[1:] + incident_bins[:-1])\n",
    "\n",
    "color_map = plt.cm.viridis(inci_values/np.max(inci_values))\n",
    "for i, e in enumerate(inci_values):\n",
    "    counts_here = counts_along_axis[i]\n",
    "    normalisation = np.sum(counts_here)*(y_values[1] - y_values[0])\n",
    "    plt.plot(y_values, counts_here/normalisation, c=color_map[i], label=f\"Incident {e}MeV\")\n",
    "\n",
    "y_values = ze_sample[..., 0][ze_sample[..., 1]>0]\n",
    "_ = plt.hist(y_values, bins=50, color='grey', alpha=0.5, histtype='step', lw=3, density=True)\n",
    "_ = plt.xlabel(\"y position after shift\")\n",
    "_ = plt.ylabel(\"Normalised counts\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1f4dd-7bc6-49cf-90ae-b071a3d24ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_bins = acc.layer_offset_bins\n",
    "incid_vs_depth = np.zeros((len(inci_values), len(shift_bins)-1))\n",
    "\n",
    "for row, (lower, upper) in enumerate(zip(incident_bins[:-1], incident_bins[1:])):\n",
    "    inci_mask = (shifts[..., 0] >= lower)*(shifts[..., 0] < upper)\n",
    "    for col, (lower, upper) in enumerate(zip(shift_bins[:-1], shift_bins[1:])):\n",
    "        shift_mask = (shifts[..., 1] >= lower)*(shifts[..., 1] < upper)\n",
    "        mask = inci_mask*shift_mask\n",
    "        energies = ze_sample[mask, :, 1]\n",
    "        incid_vs_depth[row, col] = np.sum(energies>0)\n",
    "plt.imshow(incid_vs_depth)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Depth\")\n",
    "x_ticks = [f\"{b:.2}\" for b in shift_bins]\n",
    "plt.xticks((np.arange(len(shift_bins))-0.5)[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_values]\n",
    "plt.yticks(np.arange(len(inci_values))[::2], y_ticks[::2])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"N hits\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a9db7-178c-4331-bcd2-025d5d439197",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_bins = acc.layer_offset_bins\n",
    "incid_vs_depth = np.zeros((len(inci_values), len(shift_bins)-1))\n",
    "\n",
    "for row, (lower, upper) in enumerate(zip(incident_bins[:-1], incident_bins[1:])):\n",
    "    inci_mask = (shifts[..., 0] >= lower)*(shifts[..., 0] < upper)\n",
    "    for col, (lower, upper) in enumerate(zip(shift_bins[:-1], shift_bins[1:])):\n",
    "        shift_mask = (shifts[..., 1] >= lower)*(shifts[..., 1] < upper)\n",
    "        mask = inci_mask*shift_mask\n",
    "        energies = ze_sample[mask, :, 1]\n",
    "        incid_vs_depth[row, col] = np.sum(energies>0)/np.sum(mask)\n",
    "plt.imshow(incid_vs_depth)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Depth\")\n",
    "x_ticks = [f\"{b:.2}\" for b in shift_bins]\n",
    "plt.xticks((np.arange(len(shift_bins))-0.5)[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_values]\n",
    "plt.yticks(np.arange(len(inci_values))[::2], y_ticks[::2])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"N hits per event\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84938685-a3e6-421b-b8aa-3ace954d8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_bins = acc.layer_offset_bins\n",
    "incid_vs_shift = np.zeros((len(inci_values), len(shift_bins)-1))\n",
    "\n",
    "\n",
    "\n",
    "for row, (lower, upper) in enumerate(zip(incident_bins[:-1], incident_bins[1:])):\n",
    "    print(f\"{row/len(inci_values):.0%}\", end='\\r')\n",
    "    inci_mask = (shifts[..., 0] >= lower)*(shifts[..., 0] < upper)\n",
    "    subsample = ze_sample[inci_mask]\n",
    "    for col, (lower, upper) in enumerate(zip(shift_bins[:-1], shift_bins[1:])):\n",
    "        mask = (subsample[..., 0] >= lower)*(subsample[..., 0] < upper)*(subsample[..., 1] > 0)\n",
    "        # the shifting means that some bins are only poulated by a subset of events\n",
    "        # if the shift makes the center more than one unit away ten the event does not intersect\n",
    "        num_intersecting = ((shifts[:, 1]+1) > upper)*((shifts[:, 1]-1) < lower)*inci_mask\n",
    "        incid_vs_shift[row, col] = np.sum(mask)/np.sum(num_intersecting)\n",
    "plt.imshow(incid_vs_shift)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in shift_bins]\n",
    "plt.xticks((np.arange(len(shift_bins))-0.5)[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_values]\n",
    "plt.yticks(np.arange(len(inci_values))[::2], y_ticks[::2])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"N hits per event\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83912f76-d969-41c7-b15d-cbd4ad1ba16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_bins = acc.layer_offset_bins\n",
    "depth_vs_shift = np.zeros((len(shift_bins)-1, len(shift_bins)-1))\n",
    "\n",
    "for row, (lower, upper) in enumerate(zip(shift_bins[:-1], shift_bins[1:])):\n",
    "    print(f\"{row/len(shift_bins):.0%}\", end='\\r')\n",
    "    shift_mask = (shifts[..., 1] >= lower)*(shifts[..., 1] < upper)\n",
    "    subsample = ze_sample[shift_mask]\n",
    "    for col, (lower, upper) in enumerate(zip(shift_bins[:-1], shift_bins[1:])):\n",
    "        mask = (subsample[..., 0] >= lower)*(subsample[..., 0] < upper)*(subsample[..., 1] > 0)\n",
    "        # the shifting means that some bins are only poulated by a subset of events\n",
    "        # if the shift makes the center more than one unit away ten the event does not intersect\n",
    "        num_intersecting = ((shifts[:, 1]+1) > upper)*((shifts[:, 1]-1) < lower)*shift_mask\n",
    "        depth_vs_shift[row, col] = np.sum(mask)/np.sum(num_intersecting)\n",
    "plt.imshow(depth_vs_shift)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in shift_bins]\n",
    "plt.xticks((np.arange(len(shift_bins))-0.5)[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Depth\")\n",
    "plt.yticks((np.arange(len(shift_bins))-0.5)[::5], x_ticks[::5])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"N hits\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a67fd-2bf4-4a17-b69e-d4c86dd01aee",
   "metadata": {},
   "source": [
    "## Conculsions for fitting\n",
    "\n",
    "There are no reliable strong corrilations here. For a simple model, we will ignore the impact of changing the depth of the shower,\n",
    "and consider point density to be only dependent on the incident energy and the distance from the center of the shower. \n",
    "\n",
    "Given this, we wish to calculate a mean, and a standard devation for the number of points at each incident energy and distance from the center.\n",
    "Once we have these values, two things must be done;\n",
    "\n",
    "1. Find a function (not a pdf) that can fit the mean and standard devation in two dimensions;\n",
    "\n",
    "     - This can start by plotting slices to decide if the change could be a simple rescale in either direction\n",
    "2. Use the mean and standard devation to rescale density functions for all bins of incident energy and distance, check if it's reasonable to fit them all to the same pdf after rescaling.\n",
    "3. When set up this way, it might be possible to write a super fast wish that draws all layers at once.\n",
    "\n",
    "### Starting with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b40bf-83c8-49e5-a6bd-80530197f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inci_bins = 0.5*(acc.incident_bin_boundaries[:-1] + acc.incident_bin_boundaries[1:])\n",
    "layer_bins = acc.layer_bottom\n",
    "events_per_bin = acc.total_events[1:-2]\n",
    "atleast_three = events_per_bin>3\n",
    "\n",
    "counts_per_bin = np.sum(acc.counts_hist[1:-1], axis=(2, 3))\n",
    "counts_per_event = counts_per_bin/events_per_bin\n",
    "\n",
    "plt.imshow(counts_per_event)\n",
    "plt.title(\"Measured\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in layer_bins]\n",
    "x_locs = np.arange(len(layer_bins))\n",
    "plt.xticks(x_locs[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_bins]\n",
    "y_locs = np.arange(len(inci_bins))-0.5\n",
    "plt.yticks(y_locs[::5], y_ticks[::5])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"Mean N hits per event\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "n_rows, n_cols = counts_per_event.shape\n",
    "fig, (row_ax, col_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_rows))\n",
    "for row, colour in enumerate(colours):\n",
    "    row_ax.plot(layer_bins, counts_per_event[row], c=colour)\n",
    "row_ax.set_xlabel(\"Distance to center\")\n",
    "row_ax.set_ylabel(\"mean N hits per event\")\n",
    "abs_to_center = np.abs(layer_bins)\n",
    "order_abs_to_center = np.argsort(abs_to_center)\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_cols))\n",
    "for col, colour in zip(order_abs_to_center, colours):\n",
    "    ls = '--' if layer_bins[col] < 0 else '-'\n",
    "    col_ax.plot(inci_bins, counts_per_event[:, col], c=colour, ls=ls)\n",
    "col_ax.set_xlabel(\"Incident energy\")\n",
    "col_ax.set_ylabel(\"mean N hits per event\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb7bba-f196-431a-b4d7-d8dec4998104",
   "metadata": {},
   "source": [
    "So for mean n pts per bin, the incident energy is nearly a linear fit. Treat this as a rescaling, and the distance to the center gets to be normally disributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af20f4-7a72-4709-9270-2557b10f9dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "evts_per_inci_bin = np.sum(events_per_bin, axis=1)\n",
    "counts_per_inci_bin = np.sum(acc.counts_hist[1:-1], axis=(1, 2, 3))\n",
    "inci_linear_fit = np.polyfit(inci_bins, counts_per_inci_bin/evts_per_inci_bin, 1)\n",
    "\n",
    "inci_scalings = np.polyval(inci_linear_fit, inci_bins)\n",
    "scaled_counts_per_event = counts_per_event/inci_scalings[:, np.newaxis]\n",
    "scaled_mean_counts_per_layer = np.mean(scaled_counts_per_event, axis=0)\n",
    "\n",
    "n_pts_dist_mean = np.nansum(scaled_mean_counts_per_layer*layer_bins)/np.nansum(scaled_mean_counts_per_layer)\n",
    "non_zero_weights = np.sum(scaled_mean_counts_per_layer>0)\n",
    "n_pts_dist_varience = (non_zero_weights/(non_zero_weights-1))*np.nansum(scaled_mean_counts_per_layer*(layer_bins-mean)**2)/np.nansum(scaled_mean_counts_per_layer)\n",
    "\n",
    "\n",
    "def gaussian(xs, mu, varience, height):\n",
    "    return (height/(2*np.pi))*np.exp(-(xs-mu)**2/(2*varience))\n",
    "    \n",
    "height_1_pred = gaussian(layer_bins, n_pts_dist_mean, n_pts_dist_varience, 1.)\n",
    "ratios = scaled_mean_counts_per_layer/height_1_pred\n",
    "n_pts_dist_height = np.nansum(ratios*height_1_pred)/np.nansum(height_1_pred)\n",
    "predicted_counts_per_event = gaussian(layer_bins, n_pts_dist_mean, n_pts_dist_varience, n_pts_dist_height)\n",
    "\n",
    "\n",
    "predicted_bins = np.tile(predicted_counts_per_event, (counts_per_event.shape[0], 1))\n",
    "predicted_bins *= inci_scalings[:, None]\n",
    "\n",
    "\n",
    "n_rows, n_cols = scaled_counts_per_event.shape\n",
    "fig, (row_ax, col_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_rows))\n",
    "for row, colour in enumerate(colours):\n",
    "    row_ax.plot(layer_bins, scaled_counts_per_event[row], c=colour)\n",
    "    pass\n",
    "row_ax.plot(layer_bins, scaled_mean_counts_per_layer, c=\"blue\", lw=5, alpha=0.2)\n",
    "row_ax.plot(layer_bins, predicted_counts_per_event, c=\"blue\", ls='--', lw=5, alpha=0.5)\n",
    "row_ax.set_xlabel(\"Distance to center\")\n",
    "row_ax.set_ylabel(\"rescaled mean N hits per event\")\n",
    "\n",
    "\n",
    "abs_to_center = np.abs(layer_bins)\n",
    "order_abs_to_center = np.argsort(abs_to_center)\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_cols))\n",
    "for col, colour in zip(order_abs_to_center, colours):\n",
    "    ls = '--' if layer_bins[col] < 0 else '-'\n",
    "    col_ax.plot(inci_bins, counts_per_event[:, col], c=colour, ls=ls)\n",
    "    col_ax.plot(inci_bins, predicted_bins[:, col], c=colour, ls=ls, lw=5, alpha=0.5)\n",
    "col_ax.set_xlabel(\"Incident energy\")\n",
    "col_ax.set_ylabel(\"mean N hits per event\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Predictions\")\n",
    "plt.imshow(predicted_bins)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in layer_bins]\n",
    "x_locs = np.arange(len(layer_bins))\n",
    "plt.xticks(x_locs[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_bins]\n",
    "y_locs = np.arange(len(inci_bins))-0.5\n",
    "plt.yticks(y_locs[::5], y_ticks[::5])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"Mean N hits per event\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Predictions - measured\")\n",
    "plt.imshow(predicted_bins-counts_per_event)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in layer_bins]\n",
    "x_locs = np.arange(len(layer_bins))\n",
    "plt.xticks(x_locs[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_bins]\n",
    "y_locs = np.arange(len(inci_bins))-0.5\n",
    "plt.yticks(y_locs[::5], y_ticks[::5])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"Mean N hits per event\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ae93c-186a-4a7f-ae1f-18ecfd3062e9",
   "metadata": {},
   "source": [
    "This is an acceptably good repeproduction of the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5157a-2431-440f-84e6-bc4104f8aab3",
   "metadata": {},
   "source": [
    "## Moving to standard devation\n",
    "\n",
    "or actually, coefficient of variation which is\n",
    "$$ CV = \\sigma/\\mu $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79249d57-169d-4792-b38e-d275e19c13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue with the previous accumulator\n",
    "counts_per_bin = np.sum(acc.counts_hist[1:-1], axis=(2, 3))\n",
    "counts_per_event = counts_per_bin/events_per_bin\n",
    "\n",
    "counts_sq_per_event = acc.evt_counts_sq_hist[1:-1]/events_per_bin\n",
    "stddev_counts = np.sqrt(counts_sq_per_event - counts_per_event**2)\n",
    "cv_counts = stddev_counts/counts_per_event\n",
    "\n",
    "\n",
    "plt.imshow(cv_counts)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in layer_bins]\n",
    "x_locs = np.arange(len(layer_bins))\n",
    "plt.xticks(x_locs[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_bins]\n",
    "y_locs = np.arange(len(inci_bins))-0.5\n",
    "plt.yticks(y_locs[::5], y_ticks[::5])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"coefficient of variation N hits per event\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "n_rows, n_cols = cv_counts.shape\n",
    "fig, (row_ax, col_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_rows))\n",
    "for row, colour in enumerate(colours):\n",
    "    row_ax.plot(layer_bins, cv_counts[row], c=colour)\n",
    "row_ax.set_xlabel(\"distance to center\")\n",
    "row_ax.set_ylabel(\"cv n hits per event\")\n",
    "abs_to_center = np.abs(layer_bins)\n",
    "order_abs_to_center = np.argsort(abs_to_center)\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_cols))\n",
    "for col, colour in zip(order_abs_to_center, colours):\n",
    "    ls = '--' if layer_bins[col] < 0 else '-'\n",
    "    col_ax.plot(inci_bins, cv_counts[:, col], c=colour, ls=ls)\n",
    "col_ax.set_xlabel(\"Incident energy\")\n",
    "col_ax.set_ylabel(\"CV N hits per event\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0fda9c-8b8f-4a09-aca6-cc426197cb0f",
   "metadata": {},
   "source": [
    "The coefficient of variation is more intresting than the standard devation, since the standard devation very closely follows the mean. It has no significant change in incident energy, but wants a clipped, polynomial fit in distance to center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5cd3e-1ad7-40fa-9657-894a5ad778a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv = np.nansum(cv_counts*evts_per_inci_bin[:, np.newaxis], axis=0)/np.nansum(evts_per_inci_bin)\n",
    "mask = mean_cv<0\n",
    "poly_order = 12\n",
    "fit_ys = np.copy(mean_cv)\n",
    "fit_ys[mask] = 0\n",
    "\n",
    "p0 = 10*np.ones(poly_order)\n",
    "p0[1] = -1\n",
    "\n",
    "# we need to force the fit to go down at the edges\n",
    "# simplest way is to give the fit the clipping\n",
    "def sad_poly(xs, *coeffs):\n",
    "    return np.clip(np.polyval(coeffs, xs), 0, None)\n",
    "#def sad_poly(xs, *coeffs):\n",
    "#    return np.clip(np.polynomial.chebyshev.chebval(xs, coeffs), 0, None)\n",
    "    \n",
    "cv_coeffs, _ = curve_fit(sad_poly, layer_bins, fit_ys, p0=p0, n_attempts=3, quiet=True)\n",
    "predictions = sad_poly(layer_bins, *cv_coeffs)\n",
    "\n",
    "fig, (row_ax, col_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_rows))\n",
    "for row, colour in enumerate(colours):\n",
    "    row_ax.plot(layer_bins, cv_counts[row], c=colour)\n",
    "row_ax.set_xlabel(\"distance to center\")\n",
    "row_ax.set_ylabel(\"cv n hits per event\")\n",
    "\n",
    "row_ax.plot(layer_bins, predictions, lw=5, c=\"blue\", alpha=0.5)\n",
    "\n",
    "col_ax.plot(layer_bins, predictions, lw=5, c=\"blue\", alpha=0.5)\n",
    "\n",
    "col_ax.plot(layer_bins, fit_ys, lw=1, c=\"red\")\n",
    "col_ax.set_xlabel(\"distance to center\")\n",
    "col_ax.set_ylabel(\"cv n hits per event\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda8c59-d6d6-4188-9a45-b7ff49e64d80",
   "metadata": {},
   "source": [
    "This is not a beautiful fit, but it is suficient for this model. This can be used with the mean value to find the standard devation. \n",
    "\n",
    "\n",
    "## Per bin distribution\n",
    "\n",
    "From this we must create a function that can return a mean and standard devation, given an incident energy and a distance to the center. Then the distribution of the number of hist in each bin will be rescaled, to have the same mean and standard devation, and the distributions will be stacked. Hopfully they all have the sameish Weibull distribution after rescaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07106111-239a-4d72-88e5-f31803bed922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_for_n_pts(incident_energies, distance_to_center):\n",
    "    inci_scalings = np.polyval(inci_linear_fit, incident_energies)\n",
    "    predicted_counts_per_event = gaussian(distance_to_center, n_pts_dist_mean, n_pts_dist_varience, n_pts_dist_height)\n",
    "    means = predicted_counts_per_event * inci_scalings\n",
    "    cv = sad_poly(distance_to_center, *cv_coeffs) \n",
    "    stds = means*cv\n",
    "    return means, stds\n",
    "\n",
    "def normalise_distribution(points, mean, std):\n",
    "    normed_points = points - mean\n",
    "    normed_points /= std\n",
    "    return normed_points\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da047d-c563-41b6-a832-91039131e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_distributions = [[None for _ in incident_bins[1:]] for _ in shift_bins[1:]]\n",
    "normed_distributions = [[None for _ in incident_bins[1:]] for _ in shift_bins[1:]]\n",
    "\n",
    "for row, (lower, upper) in enumerate(zip(incident_bins[:-1], incident_bins[1:])):\n",
    "    print(f\"{row/len(inci_values):.0%}\", end='\\r')\n",
    "    mean_inci = 0.5*(upper + lower)\n",
    "    inci_mask = (shifts[..., 0] >= lower)*(shifts[..., 0] < upper)\n",
    "    # the subsample is the points in the events in the incident energy bin\n",
    "    subsample = ze_sample[inci_mask]\n",
    "    for col, (lower, upper) in enumerate(zip(shift_bins[:-1], shift_bins[1:])):\n",
    "        # this mask will select both the righ layer, and the non-padding\n",
    "        mask = (subsample[..., 0] >= lower)*(subsample[..., 0] < upper)*(subsample[..., 1] > 0)\n",
    "        # the shifting means that some bins are only poulated by a subset of events\n",
    "        # if the shift makes the center more than one unit away ten the event does not intersect\n",
    "        num_intersecting = ((shifts[:, 1]+1) > upper)*((shifts[:, 1]-1) < lower)*inci_mask\n",
    "        # sum over the real points in this layer in each event, and divide by all events that could contribute to this layer\n",
    "        n_pts_in_layer_per_event = np.sum(mask, axis=1)/np.sum(num_intersecting)\n",
    "        raw_distributions[col][row] = n_pts_in_layer_per_event\n",
    "        mean, std = stats_for_n_pts(mean_inci, 0.5*(upper+lower))\n",
    "        normed_distributions[col][row] = normalise_distribution(n_pts_in_layer_per_event, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b64bf-0a94-4802-8673-6ac5deb1e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 50\n",
    "\n",
    "x_shift = 0.001\n",
    "min_x = np.log10(x_shift*0.001)\n",
    "max_x = np.log10(x_shift*1000)\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "n_slices = 5\n",
    "slices = np.linspace(0, len(raw_distributions)-1, n_slices).astype(int)\n",
    "\n",
    "fig, axarr = plt.subplots(n_slices, 2, figsize=(14, 4*n_slices))\n",
    "n_inci_bins = len(inci_bins)\n",
    "for slice_n, (ax_raw, ax_norm) in zip(slices, axarr):\n",
    "    for i in range(0, n_inci_bins):\n",
    "        color = plt.cm.magma(i/n_inci_bins)\n",
    "        \n",
    "        dist_min = raw_distributions[slice_n][i].min() - x_shift\n",
    "        dist_max = raw_distributions[slice_n][i].max() - x_shift\n",
    "        log_distribution = np.log10(raw_distributions[slice_n][i] - dist_min)\n",
    "        density = gaussian_kde(log_distribution, bw_method=0.5)\n",
    "        x_lin = np.linspace(min_x, dist_max*3, n_bins)\n",
    "        estimates = (10**density(x_lin))\n",
    "        x_log = np.logspace(min_x, dist_max*3, n_bins)\n",
    "        #ax_raw.plot(x_log, estimates, c=color)\n",
    "        ax_raw.hist(raw_distributions[slice_n][i] - dist_min, bins=x_log, alpha=0.5, color=color)\n",
    "    \n",
    "        dist_min = normed_distributions[slice_n][i].min() - x_shift\n",
    "        log_distribution = np.log10(normed_distributions[slice_n][i] - dist_min)\n",
    "        density = gaussian_kde(log_distribution, bw_method=0.5)\n",
    "        estimates = (10**density(x_lin))\n",
    "        #ax_norm.plot(x_log, estimates, c=color)\n",
    "        ax_norm.hist(normed_distributions[slice_n][i] - dist_min, bins=x_log, alpha=0.5, color=color)\n",
    "    \n",
    "    ax_raw.loglog()\n",
    "    ax_raw.set_xlabel(\"(counts in layer)/(events that intersect layer)\")\n",
    "    ax_raw.set_ylabel(f\"Density, layer {slice_n}\")\n",
    "    ax_norm.loglog()\n",
    "    ax_norm.set_xlabel(\"normed (counts in layer)/(events that intersect layer)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543f0cb-cf1e-4478-b53a-6951844b6528",
   "metadata": {},
   "source": [
    "This is far from a perfect fix, but it's still a significant improvement.\n",
    "\n",
    "## Energy\n",
    "\n",
    "Similar manipulations will allow us to obtain mean point energy distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7904cea-ad57-45d7-bdcd-d413f4104142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# chose one accumulator\n",
    "inci_bins = 0.5*(acc.incident_bin_boundaries[:-1] + acc.incident_bin_boundaries[1:])\n",
    "layer_bins = acc.layer_bottom\n",
    "events_per_bin = acc.total_events[1:-2]\n",
    "atleast_three = events_per_bin>3\n",
    "\n",
    "event_mean_e = acc.evt_mean_E_hist[1:-1]/events_per_bin\n",
    "\n",
    "\n",
    "plt.imshow(event_mean_e)\n",
    "plt.title(\"Measured\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in layer_bins]\n",
    "x_locs = np.arange(len(layer_bins))\n",
    "plt.xticks(x_locs[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_bins]\n",
    "y_locs = np.arange(len(inci_bins))-0.5\n",
    "plt.yticks(y_locs[::5], y_ticks[::5])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"Mean hit energy\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "n_rows, n_cols = event_mean_e.shape\n",
    "fig, (row_ax, col_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_rows))\n",
    "for row, colour in enumerate(colours):\n",
    "    row_ax.plot(layer_bins, event_mean_e[row], c=colour)\n",
    "row_ax.set_xlabel(\"Distance to center\")\n",
    "row_ax.set_ylabel(\"mean hit energy\")\n",
    "abs_to_center = np.abs(layer_bins)\n",
    "order_abs_to_center = np.argsort(abs_to_center)\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_cols))\n",
    "for col, colour in zip(order_abs_to_center, colours):\n",
    "    ls = '--' if layer_bins[col] < 0 else '-'\n",
    "    col_ax.plot(inci_bins, event_mean_e[:, col], c=colour, ls=ls)\n",
    "col_ax.set_xlabel(\"Incident energy\")\n",
    "col_ax.set_ylabel(\"mean hit energy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2ca7f-81e4-42e7-a788-877b6165e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_rows, n_cols = event_mean_e.shape\n",
    "fig, (row_ax, col_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_rows))\n",
    "for row, colour in enumerate(colours):\n",
    "    row_ax.plot(layer_bins, event_mean_e[row], c=colour)\n",
    "row_ax.set_xlabel(\"Distance to center\")\n",
    "row_ax.set_ylabel(\"mean hit energy\")\n",
    "abs_to_center = np.abs(layer_bins)\n",
    "order_abs_to_center = np.argsort(abs_to_center)\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_cols))\n",
    "for col, colour in zip(order_abs_to_center, colours):\n",
    "    ls = '--' if layer_bins[col] < 0 else '-'\n",
    "    col_ax.plot(inci_bins, event_mean_e[:, col], c=colour, ls=ls)\n",
    "col_ax.set_xlabel(\"Incident energy\")\n",
    "col_ax.set_ylabel(\"mean hit energy\")\n",
    "\n",
    "def gumble(xs, mu, beta, height, lift):\n",
    "    z = (xs - mu)/beta\n",
    "    return lift + height*np.exp(-z - np.exp(-z))\n",
    "\n",
    "p0 = [0., 1., 1., 0.]\n",
    "bounds = [[-10, 0., 0., -10], [10, np.inf, 10., 10]]\n",
    "found_params = np.zeros((len(inci_bins), len(p0)))\n",
    "mask = (layer_bins>-1) * (layer_bins<1.)\n",
    "xs = layer_bins[mask]\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_rows))\n",
    "for row, mean_e in enumerate(event_mean_e):\n",
    "    found_params[row], _ = curve_fit(gumble, xs, mean_e[mask], bounds=bounds, p0=p0, n_attempts=3, quiet=True)\n",
    "    predictions = gumble(layer_bins, *found_params[row])\n",
    "    row_ax.plot(layer_bins, predictions, c=colours[row], lw=5, alpha=0.6)\n",
    "row_ax.set_ylim(0.1, 1)\n",
    "row_ax.semilogy()\n",
    "\n",
    "fig, ax_arr = plt.subplots(2, 2, figsize=(10, 5))\n",
    "ax_arr = ax_arr.flatten()\n",
    "\n",
    "for i, name in enumerate([\"mu\", \"beta\", \"height\", \"lift\"]):\n",
    "    ax = ax_arr[i]\n",
    "    ax.plot(inci_values, found_params[:, i])\n",
    "    ax.set_ylabel(name)\n",
    "    ax.set_xlabel(\"Incident energy\")\n",
    "fig.tight_layout()\n",
    "\n",
    "meanE_vs_inci_mu = np.polyfit(inci_values, found_params[:, 0], 2)\n",
    "meanE_vs_inci_beta = np.polyfit(inci_values, found_params[:, 1], 2)\n",
    "meanE_vs_inci_height = np.polyfit(inci_values, found_params[:, 2], 1)\n",
    "meanE_vs_inci_lift = np.polyfit(inci_values, found_params[:, 3], 1)\n",
    "\n",
    "ax_arr[0].plot(inci_values, np.polyval(meanE_vs_inci_mu, inci_values), c='blue', lw=5, alpha=0.5)\n",
    "ax_arr[1].plot(inci_values, np.polyval(meanE_vs_inci_beta, inci_values), c='blue', lw=5, alpha=0.5)\n",
    "ax_arr[2].plot(inci_values, np.polyval(meanE_vs_inci_height, inci_values), c='blue', lw=5, alpha=0.5)\n",
    "ax_arr[3].plot(inci_values, np.polyval(meanE_vs_inci_lift, inci_values), c='blue', lw=5, alpha=0.5)\n",
    "\n",
    "def predict_event_mean_e(distances, incidents):\n",
    "    mu = np.polyval(meanE_vs_inci_mu, incidents)\n",
    "    beta = np.polyval(meanE_vs_inci_beta, incidents)\n",
    "    height = np.polyval(meanE_vs_inci_height, incidents)\n",
    "    lift = np.polyval(meanE_vs_inci_lift, incidents)\n",
    "    distances = distances.reshape(-1, 1)\n",
    "    predictions = gumble(distances, mu, beta, height, lift)\n",
    "    return predictions.T\n",
    "\n",
    "predicted_event_mean_e = predict_event_mean_e(layer_bins, inci_values)\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "image = ax.imshow(predicted_event_mean_e)\n",
    "cbar = plt.colorbar(image)\n",
    "cbar.set_label(\"Mean hit energy\")\n",
    "ax.set_title(\"Predicted\")\n",
    "ax.set_xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in layer_bins]\n",
    "x_locs = np.arange(len(layer_bins))\n",
    "plt.xticks(x_locs[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_bins]\n",
    "y_locs = np.arange(len(inci_bins))-0.5\n",
    "plt.yticks(y_locs[::5], y_ticks[::5])\n",
    "plt.tight_layout()\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "image = ax.imshow(predicted_event_mean_e - event_mean_e)\n",
    "cbar = plt.colorbar(image)\n",
    "cbar.set_label(\"Mean hit energy\")\n",
    "ax.set_title(\"Predicted - measured\")\n",
    "ax.set_xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in layer_bins]\n",
    "x_locs = np.arange(len(layer_bins))\n",
    "plt.xticks(x_locs[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_bins]\n",
    "y_locs = np.arange(len(inci_bins))-0.5\n",
    "plt.yticks(y_locs[::5], y_ticks[::5])\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5cbd8-efd6-48a5-953c-c516dd66db49",
   "metadata": {},
   "source": [
    "This is actually a really nice fit. We will continue to assume that the distribution in each bin is a log normal. For this, we also need the standard devation, or the coefficient of variation. Let's start by calculating the ocefficient of variation in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78bd8e5-228d-4656-8538-8c815b902729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "event_mean_e_sq = acc.evt_mean_E_sq_hist[1:-1]/events_per_bin\n",
    "stddev_event_mean_e = np.sqrt(event_mean_e_sq - event_mean_e**2)\n",
    "\n",
    "plt.imshow(stddev_event_mean_e)\n",
    "plt.title(\"Measured\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in layer_bins]\n",
    "x_locs = np.arange(len(layer_bins))\n",
    "plt.xticks(x_locs[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_bins]\n",
    "y_locs = np.arange(len(inci_bins))-0.5\n",
    "plt.yticks(y_locs[::5], y_ticks[::5])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"std dev Mean hit energy\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "n_rows, n_cols = event_mean_e.shape\n",
    "fig, (row_ax, col_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_rows))\n",
    "for row, colour in enumerate(colours):\n",
    "    row_ax.plot(layer_bins, stddev_event_mean_e[row], c=colour)\n",
    "row_ax.set_xlabel(\"Distance to center\")\n",
    "row_ax.set_ylabel(\"std dev mean hit energy\")\n",
    "abs_to_center = np.abs(layer_bins)\n",
    "order_abs_to_center = np.argsort(abs_to_center)\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_cols))\n",
    "for col, colour in zip(order_abs_to_center, colours):\n",
    "    ls = '--' if layer_bins[col] < 0 else '-'\n",
    "    col_ax.plot(inci_bins, stddev_event_mean_e[:, col], c=colour, ls=ls)\n",
    "col_ax.set_xlabel(\"Incident energy\")\n",
    "col_ax.set_ylabel(\"std dev mean hit energy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad30642-929d-47b3-948d-267a67d1b6f6",
   "metadata": {},
   "source": [
    "This seems like it could have the same treatment as for the coefficient of variation of the number of points, only it's the standard devation itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0e7de-4cae-41b6-83e1-433a614c7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_std = np.nansum(stddev_event_mean_e*evts_per_inci_bin[:, np.newaxis], axis=0)/np.nansum(evts_per_inci_bin)\n",
    "mask = mean_std<0\n",
    "poly_order = 12\n",
    "fit_ys = np.copy(mean_std)\n",
    "fit_ys[mask] = 0\n",
    "\n",
    "p0 = 10*np.ones(poly_order)\n",
    "p0[1] = -1\n",
    "\n",
    "# we need to force the fit to go down at the edges\n",
    "# simplest way is to give the fit the clipping\n",
    "def sad_poly(xs, *coeffs):\n",
    "    return np.clip(np.polyval(coeffs, xs), 0, None)\n",
    "#def sad_poly(xs, *coeffs):\n",
    "#    return np.clip(np.polynomial.chebyshev.chebval(xs, coeffs), 0, None)\n",
    "    \n",
    "std_event_mean_e_coeffs, _ = curve_fit(sad_poly, layer_bins, fit_ys, p0=p0, n_attempts=3, quiet=True)\n",
    "predictions = sad_poly(layer_bins, *std_event_mean_e_coeffs)\n",
    "\n",
    "fig, (row_ax, col_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_rows))\n",
    "for row, colour in enumerate(colours):\n",
    "    row_ax.plot(layer_bins, stddev_event_mean_e[row], c=colour)\n",
    "row_ax.set_xlabel(\"distance to center\")\n",
    "row_ax.set_ylabel(\"std dev mean hit energy\")\n",
    "\n",
    "row_ax.plot(layer_bins, predictions, lw=5, c=\"blue\", alpha=0.5)\n",
    "\n",
    "col_ax.plot(layer_bins, predictions, lw=5, c=\"blue\", alpha=0.5)\n",
    "\n",
    "col_ax.plot(layer_bins, fit_ys, lw=1, c=\"red\")\n",
    "col_ax.set_xlabel(\"distance to center\")\n",
    "col_ax.set_ylabel(\"std dev mean hit energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495aa04-a902-46ec-82b3-b98c19432eb8",
   "metadata": {},
   "source": [
    "Good enough.\n",
    "\n",
    "## Restacked event mean hit energy\n",
    "\n",
    "Lets use these distributions to try an restack the event mean hit energy distributions and see if they are lining up better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ecdd16-d86f-4083-918f-19cbeae87dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stats_for_event_mean_e(incident_energies, distance_to_center):\n",
    "    mu = np.polyval(meanE_vs_inci_mu, incident_energies)\n",
    "    beta = np.polyval(meanE_vs_inci_beta, incident_energies)\n",
    "    height = np.polyval(meanE_vs_inci_height, incident_energies)\n",
    "    lift = np.polyval(meanE_vs_inci_lift, incident_energies)\n",
    "    means = gumble(distance_to_center, mu, beta, height, lift)\n",
    "    stds = sad_poly(distance_to_center, *std_event_mean_e_coeffs) \n",
    "    return means, stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8a2d9-870c-4ae5-9af0-593aa9425a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_mean_e_dists = [[None for _ in incident_bins[1:]] for _ in shift_bins[1:]]\n",
    "normed_mean_e_dists = [[None for _ in incident_bins[1:]] for _ in shift_bins[1:]]\n",
    "\n",
    "for row, (lower, upper) in enumerate(zip(incident_bins[:-1], incident_bins[1:])):\n",
    "    print(f\"{row/len(inci_values):.0%}\", end='\\r')\n",
    "    mean_inci = 0.5*(upper + lower)\n",
    "    inci_mask = (shifts[..., 0] >= lower)*(shifts[..., 0] < upper)\n",
    "    # the subsample is the points in the events in the incident energy bin\n",
    "    subsample = ze_sample[inci_mask]\n",
    "    for col, (lower, upper) in enumerate(zip(shift_bins[:-1], shift_bins[1:])):\n",
    "        # this mask will select both the righ layer, and the non-padding\n",
    "        mask = (subsample[..., 0] >= lower)*(subsample[..., 0] < upper)*(subsample[..., 1] > 0)\n",
    "        # the shifting means that some bins are only poulated by a subset of events\n",
    "        # if the shift makes the center more than one unit away ten the event does not intersect\n",
    "        num_intersecting = ((shifts[:, 1]+1) > upper)*((shifts[:, 1]-1) < lower)*inci_mask\n",
    "        # sum over the real points in this layer in each event, and divide by all events that could contribute to this layer\n",
    "        mean_e_in_layer_per_event = np.sum(subsample[..., 1]*mask, axis=1)/(np.sum(num_intersecting)*np.sum(mask))\n",
    "        raw_mean_e_dists[col][row] = mean_e_in_layer_per_event\n",
    "        mean, std = stats_for_event_mean_e(mean_inci, 0.5*(upper+lower))\n",
    "        normed_mean_e_dists[col][row] = normalise_distribution(mean_e_in_layer_per_event, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83617540-5874-404b-bfb1-984ca4db674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 50\n",
    "\n",
    "x_shift = 0.00001\n",
    "min_x = np.log10(x_shift*0.1)\n",
    "max_x = np.log10(x_shift*1000)\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "n_slices = 5\n",
    "slices = np.linspace(0, len(raw_distributions)-1, n_slices).astype(int)\n",
    "\n",
    "fig, axarr = plt.subplots(n_slices, 2, figsize=(14, 4*n_slices))\n",
    "n_inci_bins = len(inci_bins)\n",
    "for slice_n, (ax_raw, ax_norm) in zip(slices, axarr):\n",
    "    for i in range(0, n_inci_bins):\n",
    "        color = plt.cm.magma(i/n_inci_bins)\n",
    "        \n",
    "        dist_min = raw_mean_e_dists[slice_n][i].min() - x_shift\n",
    "        dist_max = raw_mean_e_dists[slice_n][i].max() - x_shift\n",
    "        log_distribution = np.log10(raw_mean_e_dists[slice_n][i] - dist_min)\n",
    "        density = gaussian_kde(log_distribution, bw_method=0.5)\n",
    "        x_lin = np.linspace(min_x, dist_max - 0.9995, n_bins)\n",
    "        estimates = (10**density(x_lin))\n",
    "        #x_log = np.logspace(min_x, dist_max, n_bins)\n",
    "        x_log = np.logspace(min_x, np.log10((dist_max-dist_min)*10), n_bins)\n",
    "        #ax_raw.plot(x_log, estimates, c=color)\n",
    "        ax_raw.hist(raw_mean_e_dists[slice_n][i] - dist_min - x_shift, bins=x_log, alpha=0.5, color=color)\n",
    "    \n",
    "        dist_min = normed_mean_e_dists[slice_n][i].min() - x_shift\n",
    "        dist_max = normed_mean_e_dists[slice_n][i].max() - x_shift\n",
    "        log_distribution = np.log10(normed_mean_e_dists[slice_n][i] - dist_min)\n",
    "        density = gaussian_kde(log_distribution, bw_method=0.5)\n",
    "        estimates = (10**density(x_lin))\n",
    "        x_log = np.logspace(min_x, np.log10((dist_max-dist_min)*10), n_bins)\n",
    "        #ax_norm.plot(x_log, estimates, c=color)\n",
    "        ax_norm.hist(normed_mean_e_dists[slice_n][i] - dist_min, bins=x_log, alpha=0.5, color=color)\n",
    "\n",
    "    ax_raw.loglog()\n",
    "    ax_raw.set_xlabel(\"(mean hit energy in layer)/(events that intersect layer)\")\n",
    "    ax_raw.set_ylabel(f\"Density, layer {slice_n}\")\n",
    "    ax_norm.loglog()\n",
    "    ax_norm.set_xlabel(\"normed (mean hit energy in layer)/(events that intersect layer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb45ce1-bafa-41e5-bf7e-e63252e47667",
   "metadata": {},
   "source": [
    "Again, not super convincing, but it will have to do.\n",
    "\n",
    "## Standard devation of hit energy in event\n",
    "\n",
    "There are two remaining distributions to be fit;\n",
    "\n",
    "- The standard devation of hit energy for each event.\n",
    "- The radial distribution of the hits.\n",
    "\n",
    "The next step will be the standard devation of the hit energy, leaving the radial distribution till last.\n",
    "By now, we have a good process for generating the right fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa2df5-c5a0-42a3-a5b1-137b06395fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inci_bins = 0.5*(acc.incident_bin_boundaries[:-1] + acc.incident_bin_boundaries[1:])\n",
    "layer_bins = acc.layer_bottom\n",
    "events_per_bin = acc.total_events[1:-2]\n",
    "atleast_three = events_per_bin>3\n",
    "\n",
    "pnt_mean_E_sq = acc.pnt_mean_E_sq_hist[1:-1]/events_per_bin\n",
    "mean_pnt_E_sq = acc.evt_mean_E_sq_hist[1:-1]/events_per_bin\n",
    "std_point_E = np.sqrt(pnt_mean_E_sq - mean_pnt_E_sq)\n",
    "\n",
    "plt.imshow(std_point_E)\n",
    "plt.title(\"Measured\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in layer_bins]\n",
    "x_locs = np.arange(len(layer_bins))\n",
    "plt.xticks(x_locs[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_bins]\n",
    "y_locs = np.arange(len(inci_bins))-0.5\n",
    "plt.yticks(y_locs[::5], y_ticks[::5])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"Standard devation of energy in event\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "n_rows, n_cols = std_point_E.shape\n",
    "fig, (row_ax, col_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_rows))\n",
    "for row, colour in enumerate(colours):\n",
    "    row_ax.plot(layer_bins, std_point_E[row], c=colour)\n",
    "row_ax.set_xlabel(\"Distance to center\")\n",
    "row_ax.set_ylabel(\"Standard devation of energy in event\")\n",
    "abs_to_center = np.abs(layer_bins)\n",
    "order_abs_to_center = np.argsort(abs_to_center)\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_cols))\n",
    "for col, colour in zip(order_abs_to_center, colours):\n",
    "    ls = '--' if layer_bins[col] < 0. else '-'\n",
    "    col_ax.plot(inci_bins, std_point_E[:, col], c=colour, ls=ls)\n",
    "col_ax.set_xlabel(\"Incident energy\")\n",
    "col_ax.set_ylabel(\"Standard devation of energy in event\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c93a4-c4cb-47ef-baaa-f4a7da31bbfa",
   "metadata": {},
   "source": [
    "While it looks well like this can be treated like n-pts, linear fits to incident energy then a distance to center prediction, this won't work quite the same. The distribution is clearly not a gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215e722-4709-494f-80f4-fe75e8938160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#TOP\n",
    "\n",
    "evts_per_inci_bin = np.nansum(events_per_bin, axis=1)\n",
    "inci_bin_weights = evts_per_inci_bin/np.sum(evts_per_inci_bin)\n",
    "stdEevt_per_inci_bin = np.nansum(std_point_E * inci_bin_weights[:, None], axis=1)\n",
    "stdE_inci_linear_fit = np.polyfit(inci_bins, stdEevt_per_inci_bin, 1)\n",
    "\n",
    "stdE_inci_scalings = np.polyval(stdE_inci_linear_fit, inci_bins)\n",
    "scaled_stdE_per_event = std_point_E/stdE_inci_scalings[:, np.newaxis]\n",
    "scaled_mean_stdE_per_layer = np.nanmean(scaled_stdE_per_event, axis=0)\n",
    "\n",
    "# mask out the odd values when makeing the fits\n",
    "mask = (layer_bins > -1) * (layer_bins < 1)\n",
    "masked_layer_bins = layer_bins[mask]\n",
    "# decide on a level of lift and the height that normalises the ys\n",
    "masked_ys = scaled_mean_stdE_per_layer[mask]\n",
    "masked_ys = np.nan_to_num(masked_ys)\n",
    "\n",
    "\n",
    "stdE_dist_mean = np.nansum(masked_ys*masked_layer_bins)/np.nansum(masked_ys)\n",
    "non_zero_weights = np.nansum(masked_ys>0)\n",
    "stdE_dist_varience = (non_zero_weights/(non_zero_weights-1))*np.nansum(masked_ys*(masked_layer_bins-mean)**2)/np.nansum(masked_ys)\n",
    "\n",
    "stdE_gumbel_beta = np.sqrt(6*stdE_dist_varience)/np.pi\n",
    "stdE_gumbel_mu = stdE_dist_mean - stdE_gumbel_beta*np.euler_gamma\n",
    "\n",
    "\n",
    "def gumble(xs, mu, beta, height, lift):\n",
    "    z = (xs - mu)/beta\n",
    "    return lift + height*np.exp(-z - np.exp(-z))/beta\n",
    "\n",
    "p0 = [stdE_gumbel_mu, stdE_gumbel_beta, 1., 0]\n",
    "bounds = [[-10, 0., 0., -5.], [10., 10., 100., 0.3]]\n",
    "stdE_gumble_params, _ = curve_fit(gumble, masked_layer_bins, masked_ys, p0=p0, bounds=bounds, n_attempts=5, quiet=True)\n",
    "stdE_gumbel_mu, stdE_gumbel_beta, stdE_gumbel_height, stdE_gumbel_lift = stdE_gumble_params\n",
    "\n",
    "predicted_stdE_per_event = gumble(layer_bins, stdE_gumbel_mu, stdE_gumbel_beta, stdE_gumbel_height, stdE_gumbel_lift)\n",
    "\n",
    "\n",
    "predicted_bins = np.tile(predicted_stdE_per_event, (counts_per_event.shape[0], 1))\n",
    "predicted_bins *= stdE_inci_scalings[:, None]\n",
    "\n",
    "\n",
    "n_rows, n_cols = scaled_stdE_per_event.shape\n",
    "fig, (row_ax, col_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_rows))\n",
    "for row, colour in enumerate(colours):\n",
    "    row_ax.plot(layer_bins, scaled_stdE_per_event[row], c=colour)\n",
    "    pass\n",
    "row_ax.plot(masked_layer_bins, masked_ys, c=\"blue\", lw=10, alpha=0.2)\n",
    "row_ax.plot(layer_bins, scaled_mean_stdE_per_layer, c=\"blue\", lw=10, alpha=0.2)\n",
    "row_ax.plot(layer_bins, predicted_stdE_per_event, c=\"blue\", ls='--', lw=5, alpha=0.5)\n",
    "row_ax.set_xlabel(\"Distance to center\")\n",
    "row_ax.set_ylabel(\"rescaled std energy in event\")\n",
    "\n",
    "\n",
    "abs_to_center = np.abs(layer_bins)\n",
    "order_abs_to_center = np.argsort(abs_to_center)\n",
    "colours = plt.cm.inferno(np.linspace(0, 1, n_cols))\n",
    "for col, colour in zip(order_abs_to_center, colours):\n",
    "    ls = '--' if layer_bins[col] < 0 else '-'\n",
    "    col_ax.plot(inci_bins, std_point_E[:, col], c=colour, ls=ls)\n",
    "    col_ax.plot(inci_bins, predicted_bins[:, col], c=colour, ls=ls, lw=5, alpha=0.5)\n",
    "col_ax.set_xlabel(\"Incident energy\")\n",
    "col_ax.set_ylabel(\"std energy in event\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Predictions\")\n",
    "plt.imshow(predicted_bins)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in layer_bins]\n",
    "x_locs = np.arange(len(layer_bins))\n",
    "plt.xticks(x_locs[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_bins]\n",
    "y_locs = np.arange(len(inci_bins))-0.5\n",
    "plt.yticks(y_locs[::5], y_ticks[::5])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"Mean std energy in event\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Predictions - measured\")\n",
    "plt.imshow(predicted_bins-std_point_E)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches((10, 3))\n",
    "plt.xlabel(\"Distance to center\")\n",
    "x_ticks = [f\"{b:.2}\" for b in layer_bins]\n",
    "x_locs = np.arange(len(layer_bins))\n",
    "plt.xticks(x_locs[::5], x_ticks[::5])\n",
    "plt.ylabel(\"Incident energy\")\n",
    "y_ticks = [f\"{b:.0f}\" for b in inci_bins]\n",
    "y_locs = np.arange(len(inci_bins))-0.5\n",
    "plt.yticks(y_locs[::5], y_ticks[::5])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"Mean std energy in event\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3ea17-570d-4e48-9383-fdd5887b71f0",
   "metadata": {},
   "source": [
    "Showing the shifted distributions hasn't been that helpful so far, so I will skip that.\n",
    "Let's just do the radial distribution, then put it all together.\n",
    "\n",
    "## Radial distribution\n",
    "\n",
    "The radial distributions are complicated by two factors; 1 the gun isn't in the center, 2 the rectangular cells need a jacobean if we are \n",
    "We will start with locating the center of the distribution. Then we will caculate the radial distance to each cell, and the jacobean at each cell. From this, we can make a radial distribution to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789f9a9-a382-483b-9add-c3419941404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_xy_hits = np.sum(acc.counts_hist, axis=(0, 1))\n",
    "summed_x_hits = np.sum(acc.counts_hist, axis=(0, 1, 3))\n",
    "summed_y_hits = np.sum(acc.counts_hist, axis=(0, 1, 2))\n",
    "x_bin_centers, y_bin_centers, _ = acc._get_bin_centers()\n",
    "bin_area = acc.lateral_bin_size**2\n",
    "x_mean = np.sum(x_bin_centers*summed_x_hits)/np.sum(summed_x_hits)\n",
    "y_mean = np.sum(y_bin_centers*summed_y_hits)/np.sum(summed_y_hits)\n",
    "print(f\"x mean = {x_mean}, y mean = {y_mean}\")\n",
    "y_mean = 0.05\n",
    "shifted_y_bin_centers = y_bin_centers - y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c02017-284d-4dfd-aa1e-04b6df085b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_radii = np.sqrt(x_bin_centers[:, None]**2 + shifted_y_bin_centers**2)\n",
    "bin_jacobean = np.pi*bin_radii/bin_area\n",
    "flat_radii = bin_radii.flatten()\n",
    "all_counts = acc.counts_hist[1:-1]\n",
    "flat_counts = all_counts.reshape(*all_counts.shape[:2], -1)\n",
    "flat_counts_sq = acc.counts_sq_hist[1:-1].reshape(*all_counts.shape[:2], -1)\n",
    "unnormed_flat_probs = bin_jacobean.flatten()*flat_counts\n",
    "flat_probs = unnormed_flat_probs/np.sum(unnormed_flat_probs, axis=2)[:, :, np.newaxis]\n",
    "\n",
    "fig, (counts_ax, ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "fig.suptitle(\"Changing incident energy\")\n",
    "incident_flat_counts = np.sum(flat_counts, axis=1)\n",
    "incident_flat_probs = bin_jacobean.flatten()*incident_flat_counts\n",
    "# it's not possible to normalise this properly, as we don't have data to infinity\n",
    "# but we should still divide through by the total counts, as it's all measured in the same area\n",
    "incident_flat_probs /= np.sum(incident_flat_probs, axis=-1)[:, np.newaxis]\n",
    "colours = plt.cm.magma(np.linspace(0.01, 0.99, len(incident_flat_probs)))\n",
    "for i, c in enumerate(colours):\n",
    "    counts_ax.scatter(flat_radii, incident_flat_counts[i], c=[c], alpha=0.2)\n",
    "    ax.scatter(flat_radii, incident_flat_probs[i], c=[c], alpha=0.2)\n",
    "ax.loglog()\n",
    "counts_ax.loglog()\n",
    "counts_ax.set_ylabel(\"Counts\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "counts_ax.set_xlabel(\"Radius\")\n",
    "ax.set_xlabel(\"Radius\")\n",
    "\n",
    "fig, (counts_ax, ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "fig.suptitle(\"Changing shift from centerpoint\")\n",
    "layer_flat_counts = np.sum(flat_counts, axis=0)\n",
    "# it's not possible to normalise this, as we don't have data to infinity\n",
    "layer_flat_probs = bin_jacobean.flatten()*layer_flat_counts\n",
    "# but we should still divide through by the total counts, as it's all measured in the same area\n",
    "layer_flat_probs /= np.sum(layer_flat_probs, axis=-1)[:, np.newaxis]\n",
    "unnormed_flat_sigma = np.sqrt(np.sum(flat_counts_sq - flat_counts**2, axis=0))\n",
    "layer_flat_sigma = bin_jacobean.flatten()*unnormed_flat_sigma/np.sum(layer_flat_probs, axis=-1)[:, np.newaxis]\n",
    "colours = plt.cm.viridis(np.linspace(0.01, 0.99, len(layer_flat_probs)))\n",
    "for i, c in enumerate(colours):\n",
    "    counts_ax.scatter(flat_radii, layer_flat_counts[i], c=[c], alpha=0.2)\n",
    "    ax.scatter(flat_radii, layer_flat_probs[i], c=[c], alpha=0.2)\n",
    "ax.loglog()\n",
    "counts_ax.loglog()\n",
    "counts_ax.set_xlabel(\"Radius\")\n",
    "ax.set_xlabel(\"Radius\")\n",
    "counts_ax.set_ylabel(\"Counts\")\n",
    "ax.set_ylabel(\"Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae5ae1-ac15-4aaf-85b1-bf4c1badbc69",
   "metadata": {},
   "source": [
    "This distribution appears to be almost not energy dependent, but quite dependant on the distance from the center point.\n",
    "Typically, a two part distribution is used, one part models the center of the distribution, the other part models the tails.\n",
    "We can try the GFlash distribution, or an exponential decay. The exponential works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580dc89-32ee-434b-bd06-a13fc6f6e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointcloud.models.custom_torch_distributions import GFlashRadial\n",
    "\n",
    "def to_fit(xs, Rc, Rt_extend, p, normalisation):\n",
    "    distribution = GFlashRadial(Rc, Rt_extend, p)\n",
    "    found = distribution.prob(torch.Tensor(xs))\n",
    "    return found.detach().numpy()*normalisation\n",
    "\n",
    "def make_exponential(core, to_extention, p):\n",
    "    core = np.atleast_1d(core)\n",
    "    to_extention = np.atleast_1d(to_extention)\n",
    "    p = np.atleast_1d(p)\n",
    "    rates = np.vstack([1/core, 1/(core + to_extention)]).T\n",
    "    factors = np.vstack([p, (1-p)]).T\n",
    "    exponential = torch.distributions.Exponential(torch.Tensor(rates))\n",
    "    mix = torch.distributions.Categorical(torch.Tensor(factors))\n",
    "    combined = torch.distributions.MixtureSameFamily(mix, exponential)\n",
    "    return combined\n",
    "    \n",
    "\n",
    "def to_fit(xs, core, to_extention, p, normalisation):\n",
    "    distribution = make_exponential(core, to_extention, p)\n",
    "    found = np.exp(distribution.log_prob(torch.Tensor(xs)))\n",
    "    return found.detach().numpy()*normalisation\n",
    "\n",
    "p0 = [0.01, 0.1, 0.5, 0.1]\n",
    "bounds = [[1e-10, 1e-10, 1e-5, 0.], [10., 1000., 1-1e-5, 0.2]]\n",
    "params_per_layer = np.zeros((len(layer_flat_probs), len(p0)))\n",
    "for i, probs in enumerate(layer_flat_probs):\n",
    "    print(f\"{i/len(layer_flat_probs):.0%}\", end='\\r')\n",
    "    if np.any(np.isnan(probs)):\n",
    "        params_per_layer[i] = -1\n",
    "        continue\n",
    "    sigma = np.nan_to_num(layer_flat_sigma[i])\n",
    "    sigma[sigma <=0 ] = 1.\n",
    "    params, _ = curve_fit(to_fit, flat_radii, probs, sigma=sigma, bounds=bounds, p0=p0, n_attempts=200, quiet=True)\n",
    "    \n",
    "    params_per_layer[i] = params\n",
    "\n",
    "standard_radii = np.linspace(0, np.max(flat_radii), 20)\n",
    "\n",
    "fig, (data_ax, preds_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "fig.suptitle(\"Changing shift from centerpoint\")\n",
    "colours = plt.cm.viridis(np.linspace(0.01, 0.99, len(layer_flat_probs)))\n",
    "for i, c in enumerate(colours):\n",
    "    data_ax.scatter(flat_radii, layer_flat_probs[i], c=[c], alpha=0.2)\n",
    "    params = params_per_layer[i]\n",
    "    if np.any(params>0):\n",
    "        predictions = to_fit(standard_radii, *params)\n",
    "        preds_ax.plot(standard_radii, predictions, c=c)\n",
    "data_ax.loglog()\n",
    "preds_ax.loglog()\n",
    "preds_ax.set_xlabel(\"Radius\")\n",
    "preds_ax.set_xlabel(\"Radius\")\n",
    "data_ax.set_ylabel(\"Probability\")\n",
    "preds_ax.set_ylabel(\"Predictions\")\n",
    "preds_ax.set_ylim(1e-7, 1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8432b67-1cbc-4380-9ee8-77c83ec73c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, (data_ax, preds_ax) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "fig.suptitle(\"Changing shift from centerpoint\")\n",
    "colours = plt.cm.viridis(np.linspace(0.01, 0.99, len(layer_flat_probs)))\n",
    "for i, c in enumerate(colours):\n",
    "    data_ax.scatter(flat_radii, layer_flat_probs[i], c=[c], alpha=0.2)\n",
    "    params = params_per_layer[i]\n",
    "    if np.any(params>0):\n",
    "        predictions = to_fit(standard_radii, *params)\n",
    "        preds_ax.plot(standard_radii, predictions, c=c)\n",
    "data_ax.loglog()\n",
    "preds_ax.loglog()\n",
    "preds_ax.set_xlabel(\"Radius\")\n",
    "preds_ax.set_xlabel(\"Radius\")\n",
    "data_ax.set_ylabel(\"Probability\")\n",
    "preds_ax.set_ylabel(\"Predictions\")\n",
    "preds_ax.set_ylim(1e-7, 1.)\n",
    "data_ax.set_ylim(1e-7, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e877f6-bdff-4726-be8b-25bddd55db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axarr = plt.subplots(2, 2, figsize=(10, 8))\n",
    "for i, name in enumerate([\"core\", \"to_extended\", \"p\", \"normalisation\"]):\n",
    "    ax = axarr.flatten()[i]\n",
    "    ax.plot(layer_bins, params_per_layer[:, i])\n",
    "    ax.set_xlabel(\"Distance to center\")\n",
    "    ax.set_ylabel(name)\n",
    "\n",
    "#first_deriv = params_per_layer[1:] - params_per_layer[:-1]\n",
    "#abs_second_deriv = np.abs(first_deriv[1:] - first_deriv[:-1])\n",
    "#limit = np.mean(abs_second_deriv)/16\n",
    "#over_limit = np.any(abs_second_deriv>limit, axis=1)\n",
    "#bin_zero = np.argmin(np.abs(layer_bins))\n",
    "#half_bins_width = np.min(np.abs(np.where(over_limit)[0] - bin_zero))\n",
    "#\n",
    "#print(f\"Behavior is ok for {half_bins_width} bins either side of zero\")\n",
    "mask = np.arange(len(layer_bins))\n",
    "#mask = (mask >= (bin_zero-half_bins_width-1))*(mask <= (bin_zero+half_bins_width+1))\n",
    "\n",
    "n_radial_poly_coeffs = 2\n",
    "fitted_radial_poly = np.zeros((params_per_layer.shape[1], n_radial_poly_coeffs+1))\n",
    "masked_bins = layer_bins[mask]\n",
    "for i, name in enumerate([\"core\", \"to_extended\", \"p\", \"normalisation\"]):\n",
    "    ax = axarr.flatten()[i]\n",
    "    params = params_per_layer[:, i]\n",
    "    weights = np.ones_like(layer_bins)\n",
    "    weights[~mask] = 0.01\n",
    "    weights[params < 0] = 0\n",
    "    polycoeff = np.polyfit(layer_bins, params, n_radial_poly_coeffs, w=weights)\n",
    "    fitted_radial_poly[i] = polycoeff\n",
    "    ax.scatter(layer_bins[mask][[0, -1]], params[mask][[0, -1]], c=\"blue\", alpha=0.5, s=50)\n",
    "    ax.set_ylim(np.min(params[mask*(params>0)])*0.5, np.max(params[mask*(params>0)])*1.5)\n",
    "\n",
    "    predictions = np.polyval(polycoeff, layer_bins)\n",
    "    ax.plot(layer_bins, predictions, lw=5, c='blue', alpha=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540de17b-8b0d-486e-80a0-86e6979fdd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_radial(distance_to_center, n_pts):\n",
    "    distance_to_center = np.atleast_1d(distance_to_center)\n",
    "    core = np.polyval(fitted_radial_poly[0], distance_to_center)\n",
    "    to_extend = np.polyval(fitted_radial_poly[1], distance_to_center)\n",
    "    p = np.polyval(fitted_radial_poly[2], distance_to_center)\n",
    "    distributions = make_exponential(core, to_extend, p)\n",
    "    if isinstance(n_pts, int):\n",
    "        n_pts = torch.Size([n_pts])\n",
    "    return distributions.sample(n_pts).T\n",
    "\n",
    "\n",
    "def draw_xz(distance_to_center, n_pts):\n",
    "    if isinstance(n_pts, int):\n",
    "        n_pts = torch.Size([n_pts])\n",
    "    radii = draw_radial(distance_to_center, n_pts)\n",
    "    angular_dist = torch.distributions.Uniform(torch.tensor([-np.pi]), torch.tensor([np.pi]))\n",
    "    angles = angular_dist.sample(n_pts).T\n",
    "    xs = torch.cos(angles)*radii\n",
    "    zs = torch.sin(angles)*radii - z_mean\n",
    "    return xs, zs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7f9e3-f83b-49b2-b277-989cd3c7a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xs, zs = draw_xz(layer_bins, 100000)\n",
    "\n",
    "\n",
    "n_slices = 15\n",
    "slices = np.linspace(0, len(layer_bins), n_slices).astype(int)\n",
    "from matplotlib.colors import LogNorm\n",
    "norm = LogNorm(vmin=1e-7, vmax=1e-2)\n",
    "fig, axarr = plt.subplots(n_slices, 3, figsize=(10, n_slices*4))\n",
    "for n, (ax_real, ax, diff_ax) in enumerate(axarr):\n",
    "    hist, xedges, yedges = np.histogram2d(xs[n], zs[n], bins=[acc.lateral_x_bin_boundaries, acc.lateral_z_bin_boundaries])\n",
    "    hist /= np.sum(hist)\n",
    "    ax.imshow(hist, norm=norm)\n",
    "    real_counts = np.sum(acc.counts_hist, axis=0)[n]\n",
    "    if np.sum(real_counts):\n",
    "        real_counts /= np.sum(real_counts)\n",
    "    ax_real.imshow(real_counts, norm=norm)\n",
    "    diff_ax.imshow(np.abs(hist-real_counts), norm=norm)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3768d9-2f06-4ade-bee9-e11f375c6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904ab3c-3f2a-4026-b060-1784293bb7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caloclouds",
   "language": "python",
   "name": "caloclouds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
